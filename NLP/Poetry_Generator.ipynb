{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EsgxJ10m5ujD"},"outputs":[],"source":["import numpy as np\n","import string\n","\n","np.random.seed(1234)"]},{"cell_type":"code","source":["initial = {} # start of a phrase\n","first_order = {} # second word only\n","second_order = {}\n","third_order = {}"],"metadata":{"id":"uUXoDGlU56MW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- The first two empty strings (' ', ' ') indicate that we are not replacing any specific characters with others.\n","- The third argument, string.punctuation, specifies the characters we want to remove"],"metadata":{"id":"cyspTFs1HZis"}},{"cell_type":"code","source":["def remove_punctuation(s):\n","    return s.translate(str.maketrans('','',string.punctuation))"],"metadata":{"id":"pG8nvs2A6KGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add2dict(d, k, v):\n","  if k not in d:\n","    d[k] = []\n","  d[k].append(v)\n","\n","# [cat, cat, dog, dog, dog, dog, dog, mouse, ...]"],"metadata":{"id":"Jd5BM9nw6Y4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","for line in open('robert_frost.txt'):\n","  tokens = remove_punctuation(line.rstrip().lower()).split()\n","\n","  T = len(tokens)\n","  for i in range(T):\n","    t = tokens[i]\n","    if i == 0: # predicting 1st word\n","      # 'initial' dictstores the count of each word appearing as the first word of a line.\n","      # The keys in this dictionary are the words, and the values are the counts.\n","      initial[t] = initial.get(t, 0.) + 1\n","    else:\n","      t_1 = tokens[i-1]\n","      if i == T - 1: # predicting last word\n","        # measure probability of ending the line\n","        add2dict(second_order, (t_1, t), 'END')\n","      if i == 1:  # predicting 2nd word given 1st word\n","        # measure distribution of second word\n","        add2dict(first_order, t_1, t)\n","      else: # if not predicting 1st, 2nd or last word -> apply second_order\n","        t_2 = tokens[i-2]\n","        add2dict(second_order, (t_2, t_1), t)\n","'''"],"metadata":{"id":"mpIaDx_26a95","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1742738779059,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"f93b9bb4-fe3b-4a35-b1f8-dd1e27d448b8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfor line in open('robert_frost.txt'):\\n  tokens = remove_punctuation(line.rstrip().lower()).split()\\n\\n  T = len(tokens)\\n  for i in range(T):\\n    t = tokens[i]\\n    if i == 0: # predicting 1st word\\n      # 'initial' dictstores the count of each word appearing as the first word of a line.\\n      # The keys in this dictionary are the words, and the values are the counts.\\n      initial[t] = initial.get(t, 0.) + 1\\n    else:\\n      t_1 = tokens[i-1]\\n      if i == T - 1: # predicting last word\\n        # measure probability of ending the line\\n        add2dict(second_order, (t_1, t), 'END')\\n      if i == 1:  # predicting 2nd word given 1st word\\n        # measure distribution of second word\\n        add2dict(first_order, t_1, t)\\n      else: # if not predicting 1st, 2nd or last word -> apply second_order\\n        t_2 = tokens[i-2]\\n        add2dict(second_order, (t_2, t_1), t)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["for line in open(\"robert_frost.txt\"):\n","  tokens = remove_punctuation(line.rstrip().lower()).split()\n","\n","  T = len(tokens)\n","  for i in range(T):\n","    t = tokens[i]\n","    if i == 0:\n","      initial[t] = initial.get(t, 0.) + 1\n","    else:\n","      t_1 = tokens[i-1]\n","      if i == T - 1:\n","        add2dict(second_order, (t_1, t), 'END')\n","      if i == 1:\n","        add2dict(first_order, t_1, t)\n","      if i > 1:\n","        t_2 = tokens[i-2]\n","        add2dict(second_order, (t_2, t_1), t)\n","      if i > 2:\n","        t_3 = tokens[i-3]\n","        add2dict(third_order, (t_3, t_2, t_1), t)"],"metadata":{"id":"hh6vOGvlN6Ey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(initial.items())[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qw_gvokZKqDs","executionInfo":{"status":"ok","timestamp":1742738779066,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"02279d57-7b7d-472a-d250-fefaf7c40acc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('two', 8.0),\n"," ('and', 129.0),\n"," ('to', 50.0),\n"," ('then', 12.0),\n"," ('because', 1.0),\n"," ('though', 7.0),\n"," ('had', 4.0),\n"," ('in', 29.0),\n"," ('oh', 4.0),\n"," ('yet', 3.0)]"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["list(first_order.items())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUHQSEpIJynk","executionInfo":{"status":"ok","timestamp":1742738779070,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"8ec7ef43-7a9d-4d62-c538-94b6754d7de5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('two',\n"," ['roads', 'roads', 'miles', 'oldbelievers', 'winds', 'weeks', 'of', 'at'])"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["list(second_order.items())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vB4seRoMKo1z","executionInfo":{"status":"ok","timestamp":1742738779081,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"2e7e8e5e-ce04-4748-bf73-d5ed7419ef7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(('two', 'roads'), ['diverged', 'diverged'])"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["list(third_order.items())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BggVECcrQJ8p","executionInfo":{"status":"ok","timestamp":1742738779144,"user_tz":-330,"elapsed":62,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"10fa235e-cac7-461b-b52f-72b42d9f3331"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(('two', 'roads', 'diverged'), ['in', 'in'])"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["# normalize the distributions\n","initial_total = sum(initial.values())\n","for t, c in initial.items():\n","    initial[t] = c / initial_total"],"metadata":{"id":"UOIqCm8Q6gED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(initial.items())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhLmJgLsKme0","executionInfo":{"status":"ok","timestamp":1742738779146,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"3de4e734-e459-4e59-bb37-1a88664c8adb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('two', 0.005571030640668524)"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["# convert [cat, cat, cat, dog, dog, dog, dog, mouse, ...]\n","# into {cat: 0.5, dog: 0.4, mouse: 0.1}\n","\n","def list2pdict(ts):\n","  # turn each list of possibilities(possible tokens) into a dictionary of probabilities\n","  d = {}\n","  n = len(ts)\n","  for t in ts: # count\n","    d[t] = d.get(t, 0.) + 1\n","  for t, c in d.items(): # normalize to get probs\n","    d[t] = c / n\n","  return d"],"metadata":{"id":"-5nvUJ5z6kw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t_1, ts in first_order.items():\n","  # replace list with dictionary of probabilities\n","  first_order[t_1] = list2pdict(ts)"],"metadata":{"id":"Cw2dyiVZ60yH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, ts in second_order.items():\n","  second_order[k] = list2pdict(ts)\n","\n","for k, ts in third_order.items():\n","  third_order[k] = list2pdict(ts)"],"metadata":{"id":"Mt7enPlu65CB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_word(d):\n","  '''\n","  simulates picking a word randomly from this bag, where words with higher probabilities have a greater chance of being selected\n","  '''\n","  # dict d has tokens as keys and their corresponding probabilities as values.\n","  p0 = np.random.random()\n","  # p0 is random number between 0 and 1\n","  cumulative = 0\n","  for t, p in d.items():\n","    cumulative += p # adds the probability p of the current word t to the cumulative variable\n","    if p0 < cumulative:\n","      return t\n","  assert(False) # should never get here"],"metadata":{"id":"Br1VW7f168kn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate():\n","  for i in range(4): # generate 4 lines\n","    sentence = []\n","\n","    # initial word\n","    w0 = sample_word(initial)\n","    sentence.append(w0)\n","\n","    # sample second word\n","    w1 = sample_word(first_order[w0])\n","    sentence.append(w1)\n","\n","    if (w0, w1) in second_order:\n","      w2 = sample_word(second_order[(w0, w1)])\n","      sentence.append(w2)\n","    else:\n","      print(' '.join(sentence))\n","      continue\n","\n","    # third-order transitions until END\n","    while True:\n","      if len(sentence) > 20:\n","        break\n","      if (w0, w1, w2) in third_order:\n","        w3 = sample_word(third_order[(w0, w1, w2)])\n","        if w3 == 'END':\n","          break\n","        sentence.append(w3)\n","        w0, w1, w2 = w1, w2, w3\n","      else:\n","        break\n","    print(' '.join(sentence))"],"metadata":{"id":"f1zpAXfo7FT5","executionInfo":{"status":"ok","timestamp":1742739070260,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["generate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8Wicch67GgK","outputId":"7d8ec457-9eae-4b31-a9e3-654e1c9d22c6","executionInfo":{"status":"ok","timestamp":1742739072663,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["brown lived at such a time\n","to let on he was plagued to death with me\n","and since it was coming up had to come\n","and perhaps hear some word about the weather\n"]}]}]}