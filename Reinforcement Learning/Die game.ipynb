{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlPGr+8DCX7ECsqmoQYjnJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6aqCz0-lRjV7"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.style.use('dark_background')\n","%matplotlib inline"]},{"cell_type":"code","source":["class DieGameMDP: # class for Markov Decision Process\n","\n","  def __init__(self):\n","    # State space\n","    self.states = [0, 1] # 0 = in, 1 = out\n","\n","  def startState(self): # start state of MDP\n","    return self.states[0]\n","\n","  def isEnd(self, state): # Check if MDP has ended\n","    return state == self.states[1]\n","\n","  def actions(self, state): # possible actions from a given state\n","    action_list = []\n","    if state == self.states[0]:\n","      action_list.append('roll')\n","      action_list.append('quit')\n","    return action_list\n","\n","  def ProbReward(self, state, action=None): # return list of possible new states, correspondig porbs and rewards.\n","    newStateProbReward_list = []\n","    if action in self.actions(state):\n","      if action == 'roll':\n","        newStateProbReward_list.append((state, 2/3, 40)) # in, roll -> [3, 4, 5, 6] -> get 40 points -> continue playing\n","        newStateProbReward_list.append((state+1, 1/3, 40)) # in, roll -> [1, 2] -> get 40 points -> stop playing\n","      elif action == 'quit':\n","        newStateProbReward_list.append((state+1, 1, 100)) # in, quit -> get 100 points -> stop playing\n","    else:\n","      newStateProbReward_list.append([state, 1, 0]) # terminal state i.e out\n","    return newStateProbReward_list\n","\n","  def discount(self): # discount factor\n","    return 1"],"metadata":{"id":"GBE-39zdhAY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dieGameMDP = DieGameMDP() # object"],"metadata":{"id":"Q_OKFz5vpUsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What are the possible actions avaiable from state 0 (in) ?\n","dieGameMDP.actions(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMrCoyzPpnJs","executionInfo":{"status":"ok","timestamp":1736403178014,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"d814ead5-926d-4484-8976-70d6fa0d9815"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['roll', 'quit']"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# What are the possible actions avaiable from state 1 (out) ?\n","dieGameMDP.actions(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMQdWIo4p4l0","executionInfo":{"status":"ok","timestamp":1736403178015,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"8a13a422-da43-4523-be8b-ac060731f700"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["dieGameMDP.ProbReward(0, 'roll') # participate n roll"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VjrfQXmpqslT","executionInfo":{"status":"ok","timestamp":1736405178338,"user_tz":-330,"elapsed":497,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"eec4d67b-6895-41bc-dc8e-d852e63488ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0.6666666666666666, 40), (1, 0.3333333333333333, 40)]"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["dieGameMDP.ProbReward(0, 'quit') # participate but quit before roll"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fr3lld_UrLCT","executionInfo":{"status":"ok","timestamp":1736403178015,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"543704cf-8210-467e-cf15-77cb5d115715"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 1, 100)]"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["dieGameMDP.ProbReward(1) # dont participate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SsNZ9tOarPKj","executionInfo":{"status":"ok","timestamp":1736403178015,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"3ab95da8-52b5-4b83-a140-604bb852af54"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 1, 0]]"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# A deterministic policy (what action to take in a given state?)\n","def policy(dieGameMDP, state):\n","  # sampling the action\n","  if dieGameMDP.actions(state):\n","    return np.random.choice(['roll', 'quit'], size = 1, p = [.8, .2])"],"metadata":{"id":"HeYwnOFRrePr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.choice(range(2), size = 1, p = [2/3, 1/3]) # python alternative for R sample"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqUiKcbZzX0p","executionInfo":{"status":"ok","timestamp":1736405125010,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"f8f46fe1-819f-4403-97b0-64491e74432c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1])"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["G = 0 # cumulative reward\n","k = 0 # time stamp\n","state = dieGameMDP.startState() # start state\n","while not dieGameMDP.isEnd(state): # while not in terminal state\n","  action = policy(dieGameMDP, state)\n","  SPR_list = dieGameMDP.ProbReward(state, action) # get list of tuples\n","\n","  states = [tup[0] for tup in SPR_list]\n","  prob = [tup[1] for tup in SPR_list]\n","  reward = [tup[2] for tup in SPR_list]\n","\n","  new_state = np.random.choice(len(states), size = 1, p = prob) # get new state\n","\n","  # add rewards with discount\n","  if new_state == states[0]:\n","    G += (dieGameMDP.discount() ** k) * reward[0]\n","    print(f'State: {state}, Action: {action}, Reward: {reward[0]}, New State {new_state}')\n","  else:\n","    G += (dieGameMDP.discount() ** k) * reward[1]\n","    print(f'State: {state}, Action: roll, Reward {reward[1]}, New State {new_state}')\n","\n","  # update current state\n","  state = new_state\n","  # update time step\n","  k += 1\n","\n","print(G)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crXYMo0VxvDh","executionInfo":{"status":"ok","timestamp":1736406678937,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"2035e3c0-8421-4bea-c149-a93f07456d04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 0, Action: ['roll'], Reward: 40, New State [0]\n","State: [0], Action: ['roll'], Reward: 40, New State [0]\n","State: [0], Action: roll, Reward 40, New State [1]\n","120\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RNEUfUI014oW"},"execution_count":null,"outputs":[]}]}