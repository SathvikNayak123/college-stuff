{"cells":[{"cell_type":"markdown","metadata":{"id":"M7g7bxFCHxGP"},"source":["$${\\color{yellow}{\\text{Deep Learning for Foundations Using PyTorch}}}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0_BbyTKXQflD"},"source":["---\n","\n","Load essential libraries\n","\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"20W0d4ruQjE4","executionInfo":{"status":"ok","timestamp":1736095275935,"user_tz":-330,"elapsed":28203,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import matplotlib.pyplot as plt\n","plt.style.use('dark_background')\n","%matplotlib inline\n","import sys\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n","from sklearn.metrics import confusion_matrix\n","import gensim.downloader\n","import nltk\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"sfYXkqmLiVLM"},"source":["---\n","\n","Mount Google Drive folder if running Google Colab\n","\n","---"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VYzBBBxqiaGa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736095297300,"user_tz":-330,"elapsed":21368,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"7ce5109f-9727-4402-ec1c-339018c5decf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## Mount Google drive folder if running in Colab\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2025MAHE'\n","    DATA_DIR = DIR+'/Data/'\n","else:\n","    DATA_DIR = 'Data/'"]},{"cell_type":"markdown","metadata":{"id":"avVZ6D1ZgEUT"},"source":["---\n","\n","**We will now use Pytorch to create tensors**\n","\n","The patient data matrix:\n","\n","![patient data matrix](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=1000)\n","\n","**Notation**:\n","\n","Zeroth patient vector $\\mathbf{x}^{(0)}= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}$ and zeroth feature (heart rate vector) $\\mathbf{x}_0 = \\begin{bmatrix}72\\\\85\\\\68\\\\90\\\\84\\\\78\\end{bmatrix}.$\n","\n","---\n","\n"]},{"cell_type":"code","source":["torch.get_default_dtype()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7LUh4ah-3Kk","executionInfo":{"status":"ok","timestamp":1735971812192,"user_tz":-330,"elapsed":389,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"7eef37bf-f162-41b4-f729-be6bcdf83728"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["X = torch.Tensor(6, 5)\n","print(X.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LREdzGmm-OAl","executionInfo":{"status":"ok","timestamp":1735971701757,"user_tz":-330,"elapsed":460,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"cab8d3f8-61a1-4814-eb58-823b57334d81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.1715e-18, 8.3194e+20, 3.2797e-09, 3.3557e-06, 4.2957e-05],\n","        [1.9992e+20, 1.2858e-11, 5.4663e+22, 6.5629e-10, 5.4127e+22],\n","        [2.1707e-18, 4.5447e+30, 7.0062e+22, 2.1715e-18, 4.5447e+30],\n","        [7.0062e+22, 2.1707e-18, 1.9284e+31, 3.2314e-18, 1.5462e-04],\n","        [2.1859e-04, 1.0368e-11, 1.7490e-04, 2.6256e-06, 1.0477e+21],\n","        [1.9970e+20, 8.4030e+20, 1.6689e-07, 3.0929e-18, 3.1360e+27]])\n"]}]},{"cell_type":"code","source":["X = torch.tensor([6.0, 5])\n","X.shape\n","X.dtype"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p44LRuQl_IBm","executionInfo":{"status":"ok","timestamp":1735972010031,"user_tz":-330,"elapsed":499,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"8d042b58-b1d4-4d7f-d3b4-e1e37fd81a22"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrPnepAEvr0O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972069808,"user_tz":-330,"elapsed":549,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"31e7cc43-9632-404f-be62-972c642b9966"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n","        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n","        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n","        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n","        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n","        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]])\n","torch.Size([6, 5])\n","tensor([ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000])\n","tensor(37.3000)\n"]}],"source":["## Create a patient data matrix as a constant tensor\n","X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n","                 [85, 130, 37.0, 110, 14],\n","                 [68, 110, 38.5, 125, 34],\n","                 [90, 140, 38.0, 130, 26],\n","                 [84, 132, 38.3, 146, 30],\n","                 [78, 128, 37.2, 102, 12]])\n","print(X)\n","print(X.shape)\n","# X is a rank-2 tensor which is similar to a numpy 2D array\n","print(X[0]) # this is patient-0 info which is a rank-1 tensor\n","print(X[0, 2])"]},{"cell_type":"markdown","metadata":{"id":"cevtn_b4gek5"},"source":["---\n","\n","**Convert a PyTorch object into a numpy array**\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrYQ2moygfPu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972147811,"user_tz":-330,"elapsed":420,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"50f11ee9-f853-4de0-bfad-3746e1e1aaea"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 72.  120.   37.3 104.   32.5]\n"," [ 85.  130.   37.  110.   14. ]\n"," [ 68.  110.   38.5 125.   34. ]\n"," [ 90.  140.   38.  130.   26. ]\n"," [ 84.  132.   38.3 146.   30. ]\n"," [ 78.  128.   37.2 102.   12. ]]\n","<class 'numpy.ndarray'>\n","(6, 5)\n"]}],"source":["X_numpy = X.numpy()\n","print(X_numpy)\n","print(type(X_numpy))\n","print(X_numpy.shape)"]},{"cell_type":"markdown","metadata":{"id":"QS3MmzwsgkWU"},"source":["---\n","\n","**Addition and subtraction of vectors, scalar multiplication (apply operation componentwise)**\n","\n","![vector addition](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NokBAAAAAZLAaAoWwhtn8Vk26NotALo?width=256)\n","\n","![vector subtracton](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3M4kBAAAAAU_n_mAEv006QFZm_sUj2Dc?width=256)\n","\n","![vector multiplication](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NIkBAAAAAa_qL04bLT4kWoNeHcrR9LQ?width=256)\n","\n","![vector geometry1](https://1drv.ms/i/c/37720f927b6ddc34/IQSGNMr5z3SSRry7LSKL7LybAcGYuzgw5smabV8-6DudXIs?width=230)\n","\n","![vector geometry2](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=192)\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgPtJP0sglQP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972232829,"user_tz":-330,"elapsed":466,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"8dc49e76-1568-4fa9-d091-9518ac5454f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([153.0000, 240.0000,  75.5000, 235.0000,  48.0000])\n","tensor([ 17.0000,  20.0000,  -1.5000, -15.0000, -20.0000])\n","tensor([37.3000, 37.0000, 38.5000, 38.0000, 38.3000, 37.2000])\n","tensor([ 99.1400,  98.6000, 101.3000, 100.4000, 100.9400,  98.9600])\n","tensor([ 79.5000, 126.6667,  37.7167, 119.5000,  24.7500])\n","tensor([ 79.5000, 126.6667,  37.7167, 119.5000,  24.7500])\n"]}],"source":["# Vector addition\n","print(X[1, :] + X[2, :])\n","\n","# Vector subtraction\n","print(X[1, :] - X[2, :]) # how different patient-1 and patient-2 are\n","\n","# Scalar-vector multiplication\n","print(X[:, 2])\n","print((9/5)*X[:, 2] + 32)\n","\n","# Average patient\n","print((1/6)*(X[0, :] + X[1, :] + X[2, :] + X[3, :] + X[4, :] + X[5, :]))\n","print(torch.mean(X, dim = 0)) # dim = 0 means top-to-bottom operation or each row is an element"]},{"cell_type":"markdown","metadata":{"id":"1t_qXrlCROKA"},"source":["---\n","\n","Application of vector subtraction in natural language processing (NLP): download the word embedding model trained on Wikipedia articles.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_e13FnW0RUwy"},"outputs":[],"source":["model = gensim.downloader.load('glove-wiki-gigaword-50')"]},{"cell_type":"markdown","metadata":{"id":"7YRVJferRlK5"},"source":["---\n","\n","Now we will see what embedding vector comes as a result of applying the model for the words *cricket* and *football*.\n","\n","Next, we will do an *intuitive* subtraction of word embeddings as in\n","\n","1. Cricket without Tendulkar\n","2. Football without Messi\n","\n","Note that the embedding vectors have 50 components corresponding to the 50-dimensional embedding of model suggested by the name '**glove-wiki-gigaword-50**'\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVVFzeQyR3Wb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972256318,"user_tz":-330,"elapsed":3,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"a18ece3b-5443-4398-c0e4-6bcdefb83fc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.223 -0.283 -1.743  0.566 -0.138 -0.881 -0.269  0.419  0.95  -0.613\n","  0.009  1.005 -0.89  -0.551  0.612  0.423  0.929  0.833 -1.557  0.084\n"," -0.018  0.848  0.683  0.971  0.266 -1.054  0.407 -0.451 -0.89   0.942\n","  2.205  0.754  0.517  0.48   0.868  0.572  0.818 -0.071 -0.939 -0.816\n"," -0.355 -0.01  -0.833  1.1   -0.087  1.845 -0.831  0.437  0.63  -0.81 ]\n","[-1.821  0.701 -1.14   0.344 -0.423 -0.925 -1.394  0.285 -0.784 -0.526\n","  0.896  0.359 -0.801 -0.346  1.085 -0.087  0.634  1.143 -1.626  0.413\n"," -1.128 -0.166  0.174  0.996 -0.818 -1.772  0.078  0.134 -0.598 -0.451\n","  2.547  1.069 -0.27  -0.756  0.248  1.026  0.113  0.177 -0.233 -1.156\n"," -0.107 -0.254 -0.651  0.324 -0.583  0.881 -0.135  0.969 -0.076 -0.599]\n","[-0.772  0.413 -1.726 -0.104 -1.148 -0.855 -1.089 -0.083  0.623 -1.678\n"," -0.249 -0.492  0.188 -1.671  0.612  0.428  1.057  0.916 -0.033 -0.044\n","  0.2   -0.337  0.311  1.378 -1.137 -0.574 -0.707  0.416 -0.289  0.545\n","  1.049  0.627 -0.811 -1.277 -0.026  0.54  -0.141 -0.738 -0.305 -1.181\n","  0.057 -0.994 -0.911 -0.093  0.535  0.263 -0.63   0.645  0.778  0.151]\n","[-2.069  0.668 -1.078  0.8   -0.271 -0.263 -0.881  0.378 -0.109 -2.473\n"," -0.235 -0.584  0.104 -0.527 -0.03   0.238  0.192  1.603 -0.43   0.591\n","  0.598 -0.671  0.459  1.454 -1.156 -1.635 -1.125 -0.209 -0.008  0.255\n","  1.92   0.3    0.199 -0.675 -0.152  0.133 -0.295 -0.554 -0.31  -0.345\n"," -0.726 -1.205 -0.45   0.518  0.124  0.788 -1.134  0.914 -0.28   0.767]\n"]}],"source":["print(model['cricket'])\n","print(model['football'])\n","a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(a)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"8VPICS8ggvvg"},"source":["---\n","\n","A tensor of rank 3 corresponding to 4 time stamps (hourly), 3 samples (patients), 2 features (HR and BP)\n","\n","---"]},{"cell_type":"code","source":["# A rank-3 patient tensor with shape (4, 3, 2)\n","# with meaning for\n","# dim-0 as 4 hourly timestamps,\n","# dim-1 as 3 patients, and\n","# dim-2 as 2 features (HR and BP)\n","T = torch.tensor([[[74., 128], [79, 116], [71, 116]],\n","                 [[78, 118], [82, 124], [72, 128]],\n","                 [[84, 138], [84, 130], [74, 120]],\n","                 [[82, 126], [76, 156], [82, 132]]])\n","print(T)\n","a = torch.tensor([0.5, 0.5])\n","torch.matmul(T, a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQAvgkRkWAM8","executionInfo":{"status":"ok","timestamp":1736095662179,"user_tz":-330,"elapsed":432,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"33cfe28f-6e3c-4f52-9ef0-8b16611b156a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 74., 128.],\n","         [ 79., 116.],\n","         [ 71., 116.]],\n","\n","        [[ 78., 118.],\n","         [ 82., 124.],\n","         [ 72., 128.]],\n","\n","        [[ 84., 138.],\n","         [ 84., 130.],\n","         [ 74., 120.]],\n","\n","        [[ 82., 126.],\n","         [ 76., 156.],\n","         [ 82., 132.]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[101.0000,  97.5000,  93.5000],\n","        [ 98.0000, 103.0000, 100.0000],\n","        [111.0000, 107.0000,  97.0000],\n","        [104.0000, 116.0000, 107.0000]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qn6KT_pBgwUe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736095297300,"user_tz":-330,"elapsed":4,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"f6f10c00-7aa1-4998-89e5-c643aabc96d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 74., 128.],\n","         [ 79., 116.],\n","         [ 71., 116.]],\n","\n","        [[ 78., 118.],\n","         [ 82., 124.],\n","         [ 72., 128.]],\n","\n","        [[ 84., 138.],\n","         [ 84., 130.],\n","         [ 74., 120.]],\n","\n","        [[ 82., 126.],\n","         [ 76., 156.],\n","         [ 82., 132.]]])\n","torch.Size([4, 3, 2])\n"]}],"source":["# A rank-3 patient tensor with shape (4, 3, 2)\n","# with meaning for\n","# dim-0 as 4 hourly timestamps,\n","# dim-1 as 3 patients, and\n","# dim-2 as 2 features (HR and BP)\n","T = torch.tensor([[[74., 128], [79, 116], [71, 116]],\n","                 [[78, 118], [82, 124], [72, 128]],\n","                 [[84, 138], [84, 130], [74, 120]],\n","                 [[82, 126], [76, 156], [82, 132]]])\n","print(T)\n","print(T.shape)"]},{"cell_type":"markdown","metadata":{"id":"JV0fpSojg2EZ"},"source":["---\n","\n","**Accessing elements of a tensor**\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GbZuDYqg22n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972466097,"user_tz":-330,"elapsed":658,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"6aa6e2e4-cfd2-4616-e24d-0d6e4bdef082"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(132.)\n","tensor([[ 74., 128.],\n","        [ 79., 116.],\n","        [ 71., 116.]])\n","tensor([ 82., 132.])\n"]}],"source":["## Accessing elements of a tensor\n","# Rank-3 tensor T has axes order (timestamps, patients, features)\n","\n","# Element of T at postion 3 w.r.t. axis-0, position 2 w.r.t. axis-1,\n","# position-1 w.r.t axis-2\n","print(T[3, 2, 1]) # 3rd timestamp, 2nd patient, 1st feature (BP)\n","\n","print(T[0]) # element-0 of object T which is also the info for all patients at admission time 9AM\n","\n","print(T[3, 2]) # patient-2 info at 12PM"]},{"cell_type":"markdown","metadata":{"id":"0o6kEXfCpDzo"},"source":["---\n","\n","**Exercise**: interpret $\\texttt{T[:, -1, :]}$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6lEPZEWo6wo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972542373,"user_tz":-330,"elapsed":379,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"8bc10a61-dc5b-4b2f-a638-026400852c3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 71., 116.],\n","        [ 72., 128.],\n","        [ 74., 120.],\n","        [ 82., 132.]])"]},"metadata":{},"execution_count":83}],"source":["T[:, -1, :] # information for the last patient at all timestamps"]},{"cell_type":"markdown","metadata":{"id":"gc9EJuZQhD9i"},"source":["---\n","\n","$l_2$ norm or the geometric length of a vector denoted as $\\lVert \\mathbf{a}\\rVert$ tells us how long a vector is. In 2-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2}$$ and in $n$-dimensions, $$\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}.$$\n","\n","![vector norm](https://1drv.ms/i/c/37720f927b6ddc34/IQT817WmpQjlRqZ1R0d5Cfv6AUW6c4robL-gk06i9wmCaFU?width=500)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OM65UP4_hEso","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972591972,"user_tz":-330,"elapsed":361,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"0c3cd986-2cd5-4d06-ebfd-d2e799fe642c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 76., 124.])\n","tensor(145.4373)\n"]}],"source":["## l2 norm of a vector\n","x = torch.tensor([76., 124])\n","print(x)\n","print(torch.norm(x)) # sqrt(76^2+124^2)"]},{"cell_type":"markdown","metadata":{"id":"SRbanrUmwLX7"},"source":["\n","---\n","\n","**Dot Product of Vectors**\n","\n","A scalar resulting from an elementwise multiplication and addition: $$\\mathbf{a}{\\color{cyan}\\cdot}\\mathbf{b} = {\\color{red}{a_1b_1}}+{\\color{green}{a_2b_2}}+\\cdots+{\\color{magenta}{a_nb_n}}$$\n","\n","The <font color=\"cyan\">dot</font> ${\\color{cyan}\\cdot}$ represents the computation of the dot product.\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s91XY1JZwU2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972609731,"user_tz":-330,"elapsed":453,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"ded9c1dc-dda4-431a-b634-f4ca0e4fdc6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(32.)\n"]}],"source":["## Dot product of vectors\n","a = torch.tensor([1., 2, 3])\n","b = torch.tensor([4., 5, 6])\n","print(torch.dot(a, b)) # elementwise product followed by a summation"]},{"cell_type":"markdown","metadata":{"id":"2-b90m-QXyFp"},"source":["---\n","\n","The dot product is a measure of similarity between vectors (or, how aligned they are geometrically).\n","\n","![dot product](https://1drv.ms/i/c/37720f927b6ddc34/IQTbcGSjdbhSTJ7J39d5BCWAAWS6-y5U6J87vHuDWeAqGwM?width=6000)\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GxZ95uXXz3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972652269,"user_tz":-330,"elapsed":477,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"3f01c2e8-c8ef-4141-a666-840f6db3e204"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.)\n","tensor(0.)\n","tensor(-5.)\n"]}],"source":["a = torch.tensor([1.0, 2.0])\n","b = torch.tensor([2.0, 4.0])  # b is exactly aligned with a\n","c = torch.tensor([-2.0, 1.0]) # c is perpendicular or orthogonal to a\n","d = torch.tensor([-1.0, -2.0])  # d is anti-aligned with a\n","print(torch.dot(a, b))\n","print(torch.dot(a, c))\n","print(torch.dot(a, d))"]},{"cell_type":"markdown","metadata":{"id":"U6CS4_8byCs8"},"source":["---\n","\n","Cauchy-Schwarz inequality $-1\\leq\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\leq1.$\n","\n","This is a normalized measure of similarity (or extent of alignment) between vectors.\n","\n","Angle between vectors $\\mathbf{x}$ and $\\mathbf{y} = \\cos^{-1}\\left(\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\right).$\n","\n","![angle](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=400)\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4UhBnPUx7TV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972675812,"user_tz":-330,"elapsed":545,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"43895764-d71f-4db2-c694-d3b5a12e0654"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.8000)\n","tensor(0.6435)\n","tensor(36.8699)\n"]}],"source":["x = torch.tensor([1.0, 2.0])\n","y = torch.tensor([2.0, 1.0])\n","print(torch.dot(x, y) / (torch.norm(x) * torch.norm(y))) # normalized similarity measure\n","print(torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in radians\n","print((180/torch.pi)*torch.acos(torch.dot(x, y) / (torch.norm(x) * torch.norm(y)))) # angle in degrees"]},{"cell_type":"markdown","metadata":{"id":"1bnmEkg3Tctx"},"source":["---\n","\n","Application of the Cauchy-Schwarz inequality: is \"Cricket without Tendulkar\" same as \"Football without Messi\"?\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrmCknO5TkNZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972728248,"user_tz":-330,"elapsed":368,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"6f1370ac-8591-4ce4-f164-2d3f7a7d8d56"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7371284\n","42.51263077162803\n","4.2349043\n"]}],"source":["a = model['cricket'] - model['tendulkar']\n","b = model['football'] - model['messi']\n","print(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))) # normalized similarity\n","print((180/np.pi)*np.arccos(np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)))) # angular difference in degrees\n","print(np.linalg.norm(a-b)) # linear difference"]},{"cell_type":"markdown","metadata":{"id":"ayzM_0_synRF"},"source":["\n","---\n","\n","**Hadamard Product of Vectors**\n","\n","A vector resulting from an elementwise multiplication: $$\\mathbf{a}{\\color{cyan}\\otimes}\\mathbf{b} = \\begin{bmatrix}{\\color{red}{a_1\\times b_1}}\\\\{\\color{green}{a_2\\times b_2}}\\\\\\vdots\\\\{\\color{magenta}{a_n\\times b_n}}\\end{bmatrix}.$$\n","\n","The <font color=\"cyan\">$\\otimes$</font> represents the computation of the Hadamard product.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPojS0rIzR8p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972777119,"user_tz":-330,"elapsed":406,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"6b934fba-2ea5-4e4c-c3b1-711ee5bb60d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 4., 10., 18.])\n","tensor([ 4., 10., 18.])\n"]}],"source":["## Hadamard product\n","a = torch.tensor([1.0, 2.0, 3.0])\n","b = torch.tensor([4.0, 5.0, 6.0])\n","\n","# Element-wise multiplication (Hadamard product)\n","print(a * b)  # Using the * operator\n","print(torch.mul(a, b))  # Using torch.mul function"]},{"cell_type":"markdown","metadata":{"id":"oruyV_EjhqCR"},"source":["---\n","\n","A matrix-vector product is simply a sequence of dot products of the rows of matrix (seen as vectors) with the vector\n","\n","![matvec product](https://1drv.ms/i/c/37720f927b6ddc34/IQQ1cQ8fZdFmS4cnGkBlsZbAAaL2zMtzWdjHe-HCMt4UTA0?width=700)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_IScSWzhpi7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972820069,"user_tz":-330,"elapsed":399,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"eb9e2a40-17b0-42a3-f7f3-675be7b42801"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.,  2.,  4.],\n","        [ 2., -1.,  3.]])\n","tensor([ 4.,  2., -2.])\n","tensor([0., 0.])\n"]}],"source":["## Matrix-vector product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","x = torch.tensor([4.0, 2.0, -2.0])\n","\n","# Matrix-vector multiplication\n","print(A)\n","print(x)\n","print(torch.matmul(A, x))"]},{"cell_type":"markdown","metadata":{"id":"uTnGSJ3vT4EN"},"source":["---\n","\n","Here we create a simple sentence in English and tokenize it\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQ73kkevT5L3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972829116,"user_tz":-330,"elapsed":772,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"1265f7ea-efd5-40fe-d5ff-f6656a113c86"},"outputs":[{"output_type":"stream","name":"stdout","text":["12\n","['i', 'swam', 'quickly', 'across', 'the', 'river', 'to', 'get', 'to', 'the', 'other', 'bank']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["sentence = 'i swam quickly across the river to get to the other bank'\n","nltk.download('punkt_tab')\n","tokens = word_tokenize(sentence)\n","print(len(tokens))\n","print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"M40pqI8UUbX4"},"source":["---\n","\n","Generate the word embeddings for the tokens and store them in a matrix $\\mathbf{X}$ such that each row of the matrix corresponds to a token.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mKKVRyxUh5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972855048,"user_tz":-330,"elapsed":648,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"3a935610-081e-4595-d170-ff1d0bae047e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n","         -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04,\n","         -1.5043e-01,  8.3770e-01, -1.0797e+00, -5.1460e-01,  1.3188e+00,\n","          6.2007e-01,  1.3779e-01,  4.7108e-01, -7.2874e-02, -7.2675e-01,\n","         -7.4116e-01,  7.5263e-01,  8.8180e-01,  2.9561e-01,  1.3548e+00,\n","         -2.5701e+00, -1.3523e+00,  4.5880e-01,  1.0068e+00, -1.1856e+00,\n","          3.4737e+00,  7.7898e-01, -7.2929e-01,  2.5102e-01, -2.6156e-01,\n","         -3.4684e-01,  5.5841e-01,  7.5098e-01,  4.9830e-01, -2.6823e-01,\n","         -2.7443e-03, -1.8298e-02, -2.8096e-01,  5.5318e-01,  3.7706e-02,\n","          1.8555e-01, -1.5025e-01, -5.7512e-01, -2.6671e-01,  9.2121e-01],\n","        [-3.5303e-01,  3.6953e-01,  4.7266e-01,  1.3832e-02, -1.6484e-01,\n","         -5.2687e-01, -7.3986e-01,  1.2058e+00,  1.1147e+00, -4.6772e-01,\n","          1.7928e-01, -5.9239e-01,  2.5257e-01,  4.3449e-01,  6.7023e-01,\n","         -2.8594e-01,  7.3105e-01,  3.1828e-01, -1.5825e+00, -3.5711e-01,\n","         -1.2347e-01,  1.3207e+00,  9.0049e-01,  1.2197e-01,  1.4916e+00,\n","         -5.4105e-01,  1.2264e+00,  3.3936e-01, -3.3048e-01, -3.5457e-01,\n","         -9.6055e-02,  5.0327e-02,  1.2475e-01, -2.2816e-02, -3.5395e-01,\n","         -4.3186e-01,  3.7120e-02,  7.1439e-01, -1.5775e-02, -4.6025e-01,\n","         -3.8091e-01, -6.5412e-01,  1.2373e+00,  7.7759e-01,  6.3607e-02,\n","         -1.4376e+00,  2.4166e-01, -1.5705e+00, -1.0645e-01, -8.2711e-01],\n","        [ 5.6036e-01, -1.1257e+00,  5.5507e-01, -7.1007e-01,  5.9383e-02,\n","         -2.9997e-01, -7.8756e-01,  7.0269e-01, -2.8640e-01, -1.5299e-01,\n","          3.6951e-01,  2.8308e-01, -6.1024e-01,  5.8911e-03,  2.2412e-01,\n","          6.4999e-01, -2.3358e-01, -4.7678e-01, -1.2418e-01, -7.5310e-01,\n","          2.6407e-01, -4.9331e-02,  5.9658e-01, -2.3219e-01,  5.5754e-01,\n","         -1.5649e+00,  2.1960e-01,  2.6784e-01,  7.2161e-01, -1.3073e-01,\n","          3.0699e+00,  2.1293e-01, -1.4069e-01, -8.7782e-01, -3.6846e-01,\n","          1.1815e-01, -2.7351e-01,  5.1589e-01, -4.3990e-06, -5.4707e-01,\n","         -2.6419e-01, -2.3358e-01, -2.7178e-01, -1.6913e-01,  7.4022e-02,\n","          1.0568e-01,  8.5594e-02, -5.5750e-01, -2.7034e-01, -2.5920e-01],\n","        [ 5.2360e-01, -2.0293e-02,  2.6881e-01, -4.8425e-01, -5.4396e-01,\n","         -4.6181e-01, -9.0864e-01, -3.2993e-01,  6.2731e-01, -6.8066e-01,\n","         -5.4416e-01, -1.0720e+00,  3.9323e-02,  2.9368e-02, -4.9019e-01,\n","         -5.9847e-02,  2.3170e-01, -1.7236e-01, -6.2349e-01, -6.9779e-01,\n","          4.8163e-01,  2.1039e-01,  3.0509e-01,  5.0297e-01,  1.3997e-01,\n","         -1.2732e+00,  8.5410e-02,  7.0401e-01,  2.0331e-01, -6.5306e-01,\n","          3.6790e+00,  5.5571e-01,  5.1758e-01, -4.6839e-01, -6.0765e-01,\n","          8.2281e-02, -9.4349e-01, -3.8319e-01, -3.8270e-01,  7.0752e-01,\n","         -5.6429e-01,  4.8173e-01,  3.8864e-01,  3.8322e-02, -2.1097e-01,\n","          1.4094e-01,  1.4637e-01, -9.1432e-01, -6.1570e-01, -1.3112e+00],\n","        [ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n","        [ 7.3109e-01,  1.0242e+00, -2.6714e-01, -3.3449e-01, -1.4873e+00,\n","         -3.7840e-01, -1.2873e+00,  7.1923e-01,  1.1322e+00, -9.0059e-01,\n","         -1.2594e-01,  7.2664e-01,  7.2766e-01, -1.0212e+00, -1.6100e+00,\n","         -2.3452e-01,  1.2707e+00, -6.4512e-02, -6.8331e-01,  2.6957e-01,\n","         -2.1492e-01, -1.0354e+00,  7.7650e-01,  1.5299e-04,  6.2976e-01,\n","         -1.3818e+00, -1.9453e-01,  1.3069e+00, -3.0326e-01, -1.4769e+00,\n","          2.9696e+00, -5.6795e-01,  3.3913e-01,  3.8007e-01,  1.8625e-03,\n","         -2.8222e-01, -8.7329e-01, -7.9432e-01,  1.3823e+00,  7.9638e-01,\n","         -7.1245e-01, -2.8103e-01,  5.6754e-02, -9.7843e-01, -1.1465e+00,\n","         -6.2487e-01,  2.6281e-01, -1.4434e+00,  4.0201e-01, -1.3707e+00],\n","        [ 6.8047e-01, -3.9263e-02,  3.0186e-01, -1.7792e-01,  4.2962e-01,\n","          3.2246e-02, -4.1376e-01,  1.3228e-01, -2.9847e-01, -8.5253e-02,\n","          1.7118e-01,  2.2419e-01, -1.0046e-01, -4.3653e-01,  3.3418e-01,\n","          6.7846e-01,  5.7204e-02, -3.4448e-01, -4.2785e-01, -4.3275e-01,\n","          5.5963e-01,  1.0032e-01,  1.8677e-01, -2.6854e-01,  3.7334e-02,\n","         -2.0932e+00,  2.2171e-01, -3.9868e-01,  2.0912e-01, -5.5725e-01,\n","          3.8826e+00,  4.7466e-01, -9.5658e-01, -3.7788e-01,  2.0869e-01,\n","         -3.2752e-01,  1.2751e-01,  8.8359e-02,  1.6351e-01, -2.1634e-01,\n","         -9.4375e-02,  1.8324e-02,  2.1048e-01, -3.0880e-02, -1.9722e-01,\n","          8.2279e-02, -9.4340e-02, -7.3297e-02, -6.4699e-02, -2.6044e-01],\n","        [ 1.5910e-01, -2.1428e-01,  6.3099e-01, -5.9950e-01,  3.1248e-01,\n","         -1.6615e-01, -9.0548e-01,  4.5115e-01,  5.1568e-02,  2.5910e-01,\n","         -3.2882e-01,  4.8155e-01, -3.4982e-01,  1.2905e-01,  1.0758e+00,\n","          4.8690e-01,  5.3420e-01,  5.9762e-02,  2.1660e-01, -1.1059e+00,\n","         -2.5591e-01,  5.7462e-01,  5.4562e-01,  3.1043e-01,  3.7765e-01,\n","         -2.0337e+00, -2.2496e-01,  1.8447e-01,  8.2587e-01, -1.1991e+00,\n","          3.6042e+00,  1.1605e+00, -5.9787e-01,  1.3000e-01,  1.5678e-01,\n","          1.3166e-01,  1.8510e-01,  3.6308e-01,  5.7538e-01, -8.9593e-01,\n","         -3.6366e-01,  2.8397e-01,  4.8614e-02,  7.8780e-01, -8.7311e-02,\n","         -2.3394e-01, -1.4237e-01,  2.1215e-02, -1.4219e-01,  6.6955e-01],\n","        [ 6.8047e-01, -3.9263e-02,  3.0186e-01, -1.7792e-01,  4.2962e-01,\n","          3.2246e-02, -4.1376e-01,  1.3228e-01, -2.9847e-01, -8.5253e-02,\n","          1.7118e-01,  2.2419e-01, -1.0046e-01, -4.3653e-01,  3.3418e-01,\n","          6.7846e-01,  5.7204e-02, -3.4448e-01, -4.2785e-01, -4.3275e-01,\n","          5.5963e-01,  1.0032e-01,  1.8677e-01, -2.6854e-01,  3.7334e-02,\n","         -2.0932e+00,  2.2171e-01, -3.9868e-01,  2.0912e-01, -5.5725e-01,\n","          3.8826e+00,  4.7466e-01, -9.5658e-01, -3.7788e-01,  2.0869e-01,\n","         -3.2752e-01,  1.2751e-01,  8.8359e-02,  1.6351e-01, -2.1634e-01,\n","         -9.4375e-02,  1.8324e-02,  2.1048e-01, -3.0880e-02, -1.9722e-01,\n","          8.2279e-02, -9.4340e-02, -7.3297e-02, -6.4699e-02, -2.6044e-01],\n","        [ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n","         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n","          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n","          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n","         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n","         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n","          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n","          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n","         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n","          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n","        [ 6.4756e-01,  1.6000e-01,  2.9191e-02,  3.5118e-01,  8.9119e-02,\n","          6.1115e-01, -6.6362e-01, -5.1724e-01, -4.6521e-01, -8.8450e-02,\n","          5.0200e-02,  2.6329e-01,  1.2407e-01,  4.3832e-02,  1.7283e-01,\n","          1.3170e-02,  1.4168e-01, -1.5827e-01, -1.0427e-01, -9.3070e-01,\n","          2.1646e-01, -1.0753e-01,  6.2087e-01,  3.6761e-01, -4.8144e-01,\n","         -1.2800e+00, -5.5152e-01, -7.2023e-01, -1.7097e-01, -4.7993e-01,\n","          4.0165e+00,  4.7054e-01,  9.3614e-02, -8.6341e-01,  5.0881e-01,\n","          3.3353e-01, -3.5962e-01, -1.6648e-01, -3.1803e-01,  4.9003e-01,\n","         -3.6697e-01,  3.2051e-01,  7.0932e-01,  6.2878e-01,  7.0128e-01,\n","          1.3020e-01, -7.3769e-01,  1.0325e-01, -3.0964e-01, -4.4213e-01],\n","        [ 6.6488e-01, -1.1391e-01,  6.7844e-01,  1.7951e-01,  6.8280e-01,\n","         -4.7787e-01, -3.0761e-01,  1.7489e-01, -7.0512e-01, -5.5022e-01,\n","          1.5140e-01,  1.0214e-01, -4.5063e-01, -3.3069e-01,  5.6133e-02,\n","          1.2271e+00,  5.5607e-01, -6.8297e-01,  3.7364e-02,  7.0266e-01,\n","          1.9093e+00, -6.1483e-01, -8.3329e-01, -3.0230e-01, -1.1118e+00,\n","         -1.5500e+00,  2.6040e-01,  2.2957e-01, -1.0375e+00, -3.1789e-01,\n","          3.5091e+00, -2.5871e-01,  1.0151e+00,  6.5927e-01, -1.8231e-01,\n","         -7.5859e-01, -3.0927e-01, -9.1678e-01,  1.0633e+00, -6.6761e-01,\n","         -3.7464e-01, -2.9143e-01,  6.5606e-01, -4.4642e-01, -7.5495e-02,\n","         -1.0552e+00, -6.0501e-01,  7.3582e-01,  1.0139e+00, -2.7749e-01]])\n","torch.Size([12, 50])\n","tensor([-0.3530,  0.3695,  0.4727,  0.0138, -0.1648, -0.5269, -0.7399,  1.2058,\n","         1.1147, -0.4677,  0.1793, -0.5924,  0.2526,  0.4345,  0.6702, -0.2859,\n","         0.7311,  0.3183, -1.5825, -0.3571, -0.1235,  1.3207,  0.9005,  0.1220,\n","         1.4916, -0.5411,  1.2264,  0.3394, -0.3305, -0.3546, -0.0961,  0.0503,\n","         0.1248, -0.0228, -0.3539, -0.4319,  0.0371,  0.7144, -0.0158, -0.4602,\n","        -0.3809, -0.6541,  1.2373,  0.7776,  0.0636, -1.4376,  0.2417, -1.5705,\n","        -0.1064, -0.8271])\n"]}],"source":["X_word = torch.tensor(model[tokens])\n","np.set_printoptions(precision=3, suppress=True)\n","print(X_word)\n","print(X_word.shape)\n","print(X_word[1]) # embedding vector for the word \"swam\""]},{"cell_type":"markdown","metadata":{"id":"0Z0pZQisxtY-"},"source":["---\n","\n","A matrix-matrix product is simply a sequence of matrix-vector products.\n","\n","![matmatprod](https://1drv.ms/i/c/37720f927b6ddc34/IQQ-B3z7tbWHQqBrW9k2ElDVAUc5fWzM24txLkgBK7f8Yac?width=550)\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSg1brJ9yKnM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735972922070,"user_tz":-330,"elapsed":636,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"608928ce-a28f-48c7-fbd6-a400f2075a89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0., 11.],\n","        [ 0.,  7.]])"]},"metadata":{},"execution_count":93}],"source":["## Matrix-matrix product\n","A = torch.tensor([[1.0, 2.0, 4.0],\n","                  [2.0, -1.0, 3.0]])\n","B = torch.tensor([[4.0, -1.0],\n","                  [2.0, 0.0],\n","                  [-2.0, 3.0]])\n","torch.matmul(A, B)"]},{"cell_type":"markdown","metadata":{"id":"mVoJRc6kUtI2"},"source":["---\n","\n","The similarity between each pair of words represented in the word embeddings matrix $\\mathbf{X}_\\mathrm{word}$ is the matrix-matrix product $\\mathbf{X}_\\mathrm{word}\\mathbf{X}_\\mathrm{word}^\\mathrm{T}.$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ms9Qg5AoVJy_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735973322916,"user_tz":-330,"elapsed":669,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"9bcd64b0-70bf-46a3-f1e6-9a191a2f46bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[39.0197,  4.5574, 20.2995, 14.5481, 19.4420, 12.0143, 22.6571, 29.8528,\n","         22.6571, 19.4420, 17.8858, 11.2500],\n","        [ 4.5574, 24.7064,  4.2668,  7.2292,  0.9357,  8.8437,  2.3160,  5.4718,\n","          2.3160,  0.9357, -0.6112, -1.5239],\n","        [20.2995,  4.2668, 21.2841, 16.5796, 16.1261, 12.0684, 18.8431, 19.4523,\n","         18.8431, 16.1261, 15.2439, 12.9733],\n","        [14.5481,  7.2292, 16.5796, 28.8804, 19.3706, 22.9543, 17.9298, 16.6827,\n","         17.9298, 19.3706, 19.6938, 14.1935],\n","        [19.4420,  0.9357, 16.1261, 19.3706, 24.6793, 18.2549, 21.1141, 17.5502,\n","         21.1141, 24.6793, 19.8692, 18.0800],\n","        [12.0143,  8.8437, 12.0684, 22.9543, 18.2549, 44.2210, 14.7193, 12.0669,\n","         14.7193, 18.2549, 12.5996, 16.6162],\n","        [22.6571,  2.3160, 18.8431, 17.9298, 21.1141, 14.7193, 24.5706, 22.2478,\n","         24.5706, 21.1141, 20.6623, 19.2235],\n","        [29.8528,  5.4718, 19.4523, 16.6827, 17.5502, 12.0669, 22.2478, 30.1930,\n","         22.2478, 17.5502, 19.3905, 14.3078],\n","        [22.6571,  2.3160, 18.8431, 17.9298, 21.1141, 14.7193, 24.5706, 22.2478,\n","         24.5706, 21.1141, 20.6623, 19.2235],\n","        [19.4420,  0.9357, 16.1261, 19.3706, 24.6793, 18.2549, 21.1141, 17.5502,\n","         21.1141, 24.6793, 19.8692, 18.0800],\n","        [17.8858, -0.6112, 15.2439, 19.6938, 19.8692, 12.5996, 20.6623, 19.3905,\n","         20.6623, 19.8692, 26.9230, 15.7850],\n","        [11.2500, -1.5239, 12.9733, 14.1935, 18.0800, 16.6162, 19.2235, 14.3078,\n","         19.2235, 18.0800, 15.7850, 36.3920]])\n","torch.Size([12, 12])\n"]}],"source":["S = torch.matmul(X_word, X_word.T)\n","print(S)\n","print(S.shape)"]},{"cell_type":"markdown","metadata":{"id":"QH_sW6XW0MDT"},"source":["---\n","\n","Matrix-matrix product using q patient data matrix and a weights matrix:\n","\n","![Patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hscharGu916tjWNzZQ?embed=1&width=660)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0RFtdhxhkvZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735973351520,"user_tz":-330,"elapsed":548,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"38b62887-77fd-42ac-ecff-be4b2975d6f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 72.0000, 120.0000,  36.5000],\n","        [ 85.0000, 130.0000,  37.0000],\n","        [ 68.0000, 110.0000,  38.5000],\n","        [ 90.0000, 140.0000,  38.0000]])\n","tensor([[ 0.5000,  0.3000, -0.6000],\n","        [ 0.9000,  0.3000, -0.2500],\n","        [-1.5000,  0.4000,  0.1000]])\n","tensor([[ 89.2500,  72.2000, -69.5500],\n","        [104.0000,  79.3000, -79.8000],\n","        [ 75.2500,  68.8000, -64.4500],\n","        [114.0000,  84.2000, -85.2000]])\n"]}],"source":["# Patients data matrix\n","X = torch.tensor([[72, 120, 36.5],\n","                  [85, 130, 37.0],\n","                  [68, 110, 38.5],\n","                  [90, 140, 38.0]])\n","print(X)\n","\n","# Weights matrix\n","W = torch.tensor([[0.5, 0.3, -0.6],\n","                  [0.9, 0.3, -0.25],\n","                  [-1.5, 0.4, 0.1]])\n","print(W)\n","\n","# Raw scores matrix (matrix-matrix multiplication)\n","Z = torch.matmul(X, W) # PyTorch matmul() also does matrix-matrix multiplication\n","print(Z)\n","\n","# The raw scores are also referred to as the logits"]},{"cell_type":"markdown","metadata":{"id":"sOnZvS5Vjjrd"},"source":["---\n","\n","The softmax function\n","\n","![softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hscmdol7J2G4GDo5WQ?embed=1&width=660)\n","\n","---"]},{"cell_type":"code","source":["mysoftmax = torch.nn.Softmax(dim = 1)\n","type(mysoftmax)"],"metadata":{"id":"M7eJNfNfFQBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mysoftmax = torch.nn.functional.softmax(dim = 1)\n","type(mysoftmax)"],"metadata":{"id":"XP6xv19XFj6S"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDkPV0qXVVyp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735973455890,"user_tz":-330,"elapsed":372,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"8515106c-f1a6-4bb0-fbfe-71e1841585bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[9.9990e-01, 1.0794e-15, 7.4108e-09, 2.3555e-11, 3.1439e-09, 1.8692e-12,\n","         7.8300e-08, 1.0443e-04, 7.8300e-08, 3.1439e-09, 6.6318e-10, 8.7044e-13],\n","        [1.7758e-09, 1.0000e+00, 1.3280e-09, 2.5688e-08, 4.7482e-11, 1.2910e-07,\n","         1.8879e-10, 4.4313e-09, 1.8879e-10, 4.7482e-11, 1.0109e-11, 4.0579e-12],\n","        [2.1580e-01, 2.3505e-08, 5.7765e-01, 5.2304e-03, 3.3235e-03, 5.7455e-05,\n","         5.0300e-02, 9.2497e-02, 5.0300e-02, 3.3235e-03, 1.3755e-03, 1.4202e-04],\n","        [5.9469e-07, 3.9420e-10, 4.5349e-06, 9.9704e-01, 7.3902e-05, 2.6611e-03,\n","         1.7496e-05, 5.0274e-06, 1.7496e-05, 7.3902e-05, 1.0210e-04, 4.1714e-07],\n","        [2.5563e-03, 2.3467e-11, 9.2803e-05, 2.3801e-03, 4.8101e-01, 7.7993e-04,\n","         1.3608e-02, 3.8551e-04, 1.3608e-02, 4.8101e-01, 3.9187e-03, 6.5480e-04],\n","        [1.0299e-14, 4.3238e-16, 1.0872e-14, 5.8078e-10, 5.2853e-12, 1.0000e+00,\n","         1.5403e-13, 1.0855e-14, 1.5403e-13, 5.2853e-12, 1.8493e-14, 1.0266e-12],\n","        [6.3112e-02, 9.2493e-11, 1.3923e-03, 5.5856e-04, 1.3490e-02, 2.2532e-05,\n","         4.2770e-01, 4.1917e-02, 4.2770e-01, 1.3490e-02, 8.5862e-03, 2.0366e-03],\n","        [4.1558e-01, 1.0719e-11, 1.2640e-05, 7.9241e-07, 1.8868e-06, 7.8399e-09,\n","         2.0694e-04, 5.8397e-01, 2.0694e-04, 1.8868e-06, 1.1883e-05, 7.3716e-08],\n","        [6.3112e-02, 9.2493e-11, 1.3923e-03, 5.5856e-04, 1.3490e-02, 2.2532e-05,\n","         4.2770e-01, 4.1917e-02, 4.2770e-01, 1.3490e-02, 8.5862e-03, 2.0366e-03],\n","        [2.5563e-03, 2.3467e-11, 9.2803e-05, 2.3801e-03, 4.8101e-01, 7.7993e-04,\n","         1.3608e-02, 3.8551e-04, 1.3608e-02, 4.8101e-01, 3.9187e-03, 6.5480e-04],\n","        [1.1808e-04, 1.0940e-12, 8.4105e-06, 7.2006e-04, 8.5813e-04, 5.9757e-07,\n","         1.8967e-03, 5.3167e-04, 1.8967e-03, 8.5813e-04, 9.9310e-01, 1.4448e-05],\n","        [1.2049e-11, 3.4143e-17, 6.7511e-11, 2.2871e-10, 1.1148e-08, 2.5790e-09,\n","         3.4977e-08, 2.5642e-10, 3.4977e-08, 1.1148e-08, 1.1233e-09, 1.0000e+00]])\n"]}],"source":["## In-built softmax function in PyTorch (dim = 1 corresponds to applying row-by-row)\n","## applied to the word embeddings similarity matrix\n","S_softmax = torch.nn.functional.softmax(S, dim = 1)\n","print(S_softmax)"]},{"cell_type":"markdown","metadata":{"id":"_OVqwridV5T_"},"source":["---\n","\n","Transform the word embeddings using the softmax-normalized similarity matrix.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SF731yR3V3E7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735973792769,"user_tz":-330,"elapsed":652,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"71ea2727-2295-4937-cf80-fdc37430501d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.1891e-01,  1.5251e-01, -8.1998e-02, -7.4142e-01,  7.5912e-01,\n","         -4.8325e-01, -3.1015e-01,  5.1475e-01, -9.8697e-01,  6.4455e-04,\n","         -1.5045e-01,  8.3766e-01, -1.0796e+00, -5.1453e-01,  1.3188e+00,\n","          6.2006e-01,  1.3783e-01,  4.7104e-01, -7.2844e-02, -7.2679e-01,\n","         -7.4111e-01,  7.5261e-01,  8.8176e-01,  2.9561e-01,  1.3547e+00,\n","         -2.5700e+00, -1.3522e+00,  4.5877e-01,  1.0068e+00, -1.1856e+00,\n","          3.4737e+00,  7.7902e-01, -7.2928e-01,  2.5101e-01, -2.6152e-01,\n","         -3.4679e-01,  5.5837e-01,  7.5094e-01,  4.9831e-01, -2.6830e-01,\n","         -2.7820e-03, -1.8266e-02, -2.8093e-01,  5.5320e-01,  3.7693e-02,\n","          1.8551e-01, -1.5025e-01, -5.7506e-01, -2.6670e-01,  9.2118e-01],\n","        [-3.5303e-01,  3.6953e-01,  4.7266e-01,  1.3832e-02, -1.6484e-01,\n","         -5.2687e-01, -7.3986e-01,  1.2058e+00,  1.1147e+00, -4.6772e-01,\n","          1.7928e-01, -5.9239e-01,  2.5257e-01,  4.3449e-01,  6.7023e-01,\n","         -2.8594e-01,  7.3105e-01,  3.1828e-01, -1.5825e+00, -3.5711e-01,\n","         -1.2347e-01,  1.3207e+00,  9.0049e-01,  1.2197e-01,  1.4916e+00,\n","         -5.4105e-01,  1.2264e+00,  3.3936e-01, -3.3048e-01, -3.5457e-01,\n","         -9.6054e-02,  5.0327e-02,  1.2475e-01, -2.2816e-02, -3.5395e-01,\n","         -4.3186e-01,  3.7120e-02,  7.1439e-01, -1.5775e-02, -4.6025e-01,\n","         -3.8091e-01, -6.5412e-01,  1.2373e+00,  7.7759e-01,  6.3607e-02,\n","         -1.4376e+00,  2.4166e-01, -1.5705e+00, -1.0645e-01, -8.2711e-01],\n","        [ 4.3907e-01, -6.3930e-01,  3.9044e-01, -6.4476e-01,  2.6984e-01,\n","         -2.9165e-01, -6.5632e-01,  5.6847e-01, -4.0110e-01, -8.1029e-02,\n","          1.6688e-01,  4.0522e-01, -6.3131e-01, -1.3854e-01,  5.4471e-01,\n","          6.2251e-01, -4.7790e-02, -2.0495e-01, -1.1952e-01, -7.4331e-01,\n","          2.8192e-02,  1.9576e-01,  6.0551e-01, -6.7079e-02,  6.5179e-01,\n","         -1.8785e+00, -1.6886e-01,  2.3414e-01,  7.2940e-01, -5.0384e-01,\n","          3.2990e+00,  4.4844e-01, -3.9066e-01, -4.8452e-01, -2.3629e-01,\n","         -2.6554e-02, -1.1883e-02,  4.9907e-01,  1.7507e-01, -4.7457e-01,\n","         -2.0187e-01, -1.0891e-01, -1.9116e-01,  9.2081e-02,  1.9823e-02,\n","          8.9705e-02, -5.9408e-03, -4.5740e-01, -2.3762e-01,  7.1990e-02],\n","        [ 5.2415e-01, -1.7462e-02,  2.6726e-01, -4.8367e-01, -5.4623e-01,\n","         -4.6140e-01, -9.0954e-01, -3.2711e-01,  6.2841e-01, -6.8115e-01,\n","         -5.4283e-01, -1.0669e+00,  4.1065e-02,  2.6575e-02, -4.9299e-01,\n","         -6.0261e-02,  2.3443e-01, -1.7207e-01, -6.2361e-01, -6.9515e-01,\n","          4.7967e-01,  2.0696e-01,  3.0631e-01,  5.0148e-01,  1.4116e-01,\n","         -1.2736e+00,  8.4477e-02,  7.0534e-01,  2.0184e-01, -6.5516e-01,\n","          3.6772e+00,  5.5260e-01,  5.1685e-01, -4.6614e-01, -6.0579e-01,\n","          8.1311e-02, -9.4303e-01, -3.8420e-01, -3.7791e-01,  7.0757e-01,\n","         -5.6461e-01,  4.7957e-01,  3.8767e-01,  3.5665e-02, -2.1340e-01,\n","          1.3890e-01,  1.4656e-01, -9.1548e-01, -6.1286e-01, -1.3111e+00],\n","        [ 4.2585e-01,  2.4063e-01, -3.8746e-01,  1.1012e-01,  3.4426e-01,\n","         -4.2530e-02, -4.9645e-01, -1.6882e-01, -1.1197e-02, -6.3692e-01,\n","          2.7094e-01, -1.3449e-01, -5.4045e-01,  1.2709e-01,  2.0455e-03,\n","          3.2044e-02,  1.0272e-01, -1.3278e-01, -8.2639e-01, -1.3595e-01,\n","          1.7693e-04, -3.1641e-01, -1.3848e-01, -2.2654e-01, -1.8164e-01,\n","         -1.8854e+00, -7.3774e-01,  8.5729e-02, -3.9770e-01, -2.1131e-01,\n","          4.0002e+00, -1.6095e-01, -5.2862e-01, -3.1822e-01,  6.0369e-03,\n","         -1.7909e-03,  1.7143e-01, -1.5120e-01,  1.7147e-02, -5.5345e-02,\n","         -2.9369e-01, -1.4898e-01, -3.2519e-01, -4.1542e-02, -4.2970e-01,\n","          1.8302e-01, -3.0532e-03, -1.8304e-01, -1.1499e-01, -7.6656e-01],\n","        [ 7.3109e-01,  1.0242e+00, -2.6714e-01, -3.3449e-01, -1.4873e+00,\n","         -3.7840e-01, -1.2873e+00,  7.1923e-01,  1.1322e+00, -9.0059e-01,\n","         -1.2594e-01,  7.2664e-01,  7.2766e-01, -1.0212e+00, -1.6100e+00,\n","         -2.3452e-01,  1.2707e+00, -6.4512e-02, -6.8331e-01,  2.6957e-01,\n","         -2.1492e-01, -1.0354e+00,  7.7650e-01,  1.5299e-04,  6.2976e-01,\n","         -1.3818e+00, -1.9453e-01,  1.3069e+00, -3.0326e-01, -1.4769e+00,\n","          2.9696e+00, -5.6795e-01,  3.3913e-01,  3.8007e-01,  1.8625e-03,\n","         -2.8222e-01, -8.7329e-01, -7.9432e-01,  1.3823e+00,  7.9638e-01,\n","         -7.1245e-01, -2.8103e-01,  5.6754e-02, -9.7843e-01, -1.1465e+00,\n","         -6.2487e-01,  2.6281e-01, -1.4434e+00,  4.0201e-01, -1.3707e+00],\n","        [ 6.1553e-01, -2.6617e-02,  2.7090e-01, -2.1872e-01,  4.3972e-01,\n","         -7.4917e-03, -4.3282e-01,  1.5646e-01, -3.2092e-01, -8.2234e-02,\n","          1.3161e-01,  2.6312e-01, -1.8442e-01, -3.9681e-01,  4.1553e-01,\n","          6.4369e-01,  8.4955e-02, -2.6939e-01, -3.8564e-01, -4.7367e-01,\n","          4.2713e-01,  1.4627e-01,  2.3875e-01, -2.0178e-01,  1.2257e-01,\n","         -2.1058e+00,  7.0311e-02, -3.0659e-01,  2.6320e-01, -6.1237e-01,\n","          3.8476e+00,  5.0292e-01, -9.0048e-01, -3.1804e-01,  1.7174e-01,\n","         -2.9481e-01,  1.5222e-01,  1.3110e-01,  1.9501e-01, -2.3850e-01,\n","         -1.0882e-01,  2.4269e-02,  1.6224e-01,  4.4542e-02, -1.7609e-01,\n","          7.6534e-02, -1.0343e-01, -1.0201e-01, -8.2547e-02, -1.6326e-01],\n","        [ 1.4263e-01, -6.1765e-02,  3.3451e-01, -6.5830e-01,  4.9816e-01,\n","         -2.9785e-01, -6.5784e-01,  4.7744e-01, -3.8023e-01,  1.5152e-01,\n","         -2.5446e-01,  6.2944e-01, -6.5304e-01, -1.3868e-01,  1.1765e+00,\n","          5.4232e-01,  3.6925e-01,  2.3052e-01,  9.6020e-02, -9.4804e-01,\n","         -4.5722e-01,  6.4838e-01,  6.8518e-01,  3.0402e-01,  7.8358e-01,\n","         -2.2566e+00, -6.9328e-01,  2.9823e-01,  9.0079e-01, -1.1932e+00,\n","          3.5501e+00,  1.0016e+00, -6.5262e-01,  1.8006e-01, -1.7057e-02,\n","         -6.7384e-02,  3.4020e-01,  5.2416e-01,  5.4316e-01, -6.3476e-01,\n","         -2.1356e-01,  1.5824e-01, -8.8281e-02,  6.8994e-01, -3.5392e-02,\n","         -5.9466e-02, -1.4563e-01, -2.2666e-01, -1.9391e-01,  7.7372e-01],\n","        [ 6.1553e-01, -2.6617e-02,  2.7090e-01, -2.1872e-01,  4.3972e-01,\n","         -7.4917e-03, -4.3282e-01,  1.5646e-01, -3.2092e-01, -8.2234e-02,\n","          1.3161e-01,  2.6312e-01, -1.8442e-01, -3.9681e-01,  4.1553e-01,\n","          6.4369e-01,  8.4955e-02, -2.6939e-01, -3.8564e-01, -4.7367e-01,\n","          4.2713e-01,  1.4627e-01,  2.3875e-01, -2.0178e-01,  1.2257e-01,\n","         -2.1058e+00,  7.0311e-02, -3.0659e-01,  2.6320e-01, -6.1237e-01,\n","          3.8476e+00,  5.0292e-01, -9.0048e-01, -3.1804e-01,  1.7174e-01,\n","         -2.9481e-01,  1.5222e-01,  1.3110e-01,  1.9501e-01, -2.3850e-01,\n","         -1.0882e-01,  2.4269e-02,  1.6224e-01,  4.4542e-02, -1.7609e-01,\n","          7.6534e-02, -1.0343e-01, -1.0201e-01, -8.2547e-02, -1.6326e-01],\n","        [ 4.2585e-01,  2.4063e-01, -3.8746e-01,  1.1012e-01,  3.4426e-01,\n","         -4.2530e-02, -4.9645e-01, -1.6882e-01, -1.1197e-02, -6.3692e-01,\n","          2.7094e-01, -1.3449e-01, -5.4045e-01,  1.2709e-01,  2.0455e-03,\n","          3.2044e-02,  1.0272e-01, -1.3278e-01, -8.2639e-01, -1.3595e-01,\n","          1.7693e-04, -3.1641e-01, -1.3848e-01, -2.2654e-01, -1.8164e-01,\n","         -1.8854e+00, -7.3774e-01,  8.5729e-02, -3.9770e-01, -2.1131e-01,\n","          4.0002e+00, -1.6095e-01, -5.2862e-01, -3.1822e-01,  6.0369e-03,\n","         -1.7909e-03,  1.7143e-01, -1.5120e-01,  1.7147e-02, -5.5345e-02,\n","         -2.9369e-01, -1.4898e-01, -3.2519e-01, -4.1542e-02, -4.2970e-01,\n","          1.8302e-01, -3.0532e-03, -1.8304e-01, -1.1499e-01, -7.6656e-01],\n","        [ 6.4688e-01,  1.5905e-01,  2.9960e-02,  3.4753e-01,  9.0600e-02,\n","          6.0649e-01, -6.6265e-01, -5.1340e-01, -4.6278e-01, -8.9652e-02,\n","          5.0401e-02,  2.6166e-01,  1.2158e-01,  4.2149e-02,  1.7326e-01,\n","          1.5985e-02,  1.4157e-01, -1.5875e-01, -1.0697e-01, -9.2730e-01,\n","          2.1721e-01, -1.0644e-01,  6.1763e-01,  3.6421e-01, -4.7785e-01,\n","         -1.2847e+00, -5.4840e-01, -7.1594e-01, -1.6902e-01, -4.8032e-01,\n","          4.0154e+00,  4.6988e-01,  8.8424e-02, -8.5966e-01,  5.0570e-01,\n","          3.3008e-01, -3.5687e-01, -1.6527e-01, -3.1509e-01,  4.8572e-01,\n","         -3.6592e-01,  3.1859e-01,  7.0491e-01,  6.2475e-01,  6.9474e-01,\n","          1.2992e-01, -7.3295e-01,  1.0123e-01, -3.0848e-01, -4.4190e-01],\n","        [ 6.6488e-01, -1.1391e-01,  6.7844e-01,  1.7951e-01,  6.8280e-01,\n","         -4.7787e-01, -3.0761e-01,  1.7489e-01, -7.0512e-01, -5.5022e-01,\n","          1.5140e-01,  1.0214e-01, -4.5063e-01, -3.3069e-01,  5.6133e-02,\n","          1.2271e+00,  5.5607e-01, -6.8297e-01,  3.7364e-02,  7.0266e-01,\n","          1.9093e+00, -6.1483e-01, -8.3329e-01, -3.0230e-01, -1.1118e+00,\n","         -1.5500e+00,  2.6040e-01,  2.2957e-01, -1.0375e+00, -3.1789e-01,\n","          3.5091e+00, -2.5871e-01,  1.0151e+00,  6.5927e-01, -1.8231e-01,\n","         -7.5859e-01, -3.0927e-01, -9.1678e-01,  1.0633e+00, -6.6761e-01,\n","         -3.7464e-01, -2.9143e-01,  6.5606e-01, -4.4642e-01, -7.5495e-02,\n","         -1.0552e+00, -6.0501e-01,  7.3582e-01,  1.0139e+00, -2.7749e-01]])\n"]}],"source":["X_word = torch.tensor(model[tokens])\n","Y = torch.matmul(S_softmax, X_word)\n","print(Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EG_bqPgjlV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735973841942,"user_tz":-330,"elapsed":443,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"ba3f9377-5e10-4019-e86d-3f7abd17e6a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 89.2500,  72.2000, -69.5500],\n","        [104.0000,  79.3000, -79.8000],\n","        [ 75.2500,  68.8000, -64.4500],\n","        [114.0000,  84.2000, -85.2000]])\n","tensor([[1.0000e+00, 3.9380e-08, 0.0000e+00],\n","        [1.0000e+00, 1.8747e-11, 0.0000e+00],\n","        [9.9842e-01, 1.5780e-03, 0.0000e+00],\n","        [1.0000e+00, 1.1429e-13, 0.0000e+00]])\n"]}],"source":["## In-built softmax function in PyTorch (dim = 1 corresponds to row-by-row)\n","## applied to the toy patient data matrix\n","print(Z)\n","softmax_scores = torch.nn.functional.softmax(Z, dim = 1)\n","print(softmax_scores)"]},{"cell_type":"markdown","metadata":{"id":"6_8cPXFFkiUZ"},"source":["---\n","\n","A toy data matrix with output labels and an initial weights matrix for the softmax classifier:\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJ3U-JCukmIG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974060525,"user_tz":-330,"elapsed":715,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"91194b20-7cfe-4255-a0bc-eb1441f14910"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920],\n","        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374],\n","        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647],\n","        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439],\n","        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043],\n","        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676]])\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]])\n","tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000]])\n"]}],"source":["# Create the data matrix (read from a file typically)\n","X = np.array([[72, 120, 37.3, 104, 32.5],\n","              [85, 130, 37.0, 110, 14],\n","              [68, 110, 38.5, 125, 34],\n","              [90, 140, 38.0, 130, 26],\n","              [84, 132, 38.3, 146, 30],\n","              [78, 128, 37.2, 102, 12]])\n","\n","# Standardize the data matrix\n","sc = StandardScaler()\n","X_S = sc.fit_transform(X)  # fit(), fit_transform(), transform()\n","\n","# Convert to a PyTorch tensor\n","X_S = torch.tensor(X_S, dtype = torch.float32)\n","\n","# Get the number of samples and features\n","num_samples, num_features = X_S.shape\n","\n","# Create the output labels vector (also read from a file typically)\n","y = np.array(['non-diabetic',\n","              'diabetic',\n","              'non-diabetic',\n","              'pre-diabetic',\n","              'diabetic',\n","              'pre-diabetic'])\n","\n","# One-hot encoding of output labels using scikit-learn\n","ohe = OneHotEncoder(sparse_output = False)  # Use `sparse_output=False` for dense array\n","Y = ohe.fit_transform(y.reshape(-1, 1))\n","\n","# Convert to a PyTorch tensor\n","Y = torch.tensor(Y, dtype = torch.float32)\n","\n","# Get the number of labels\n","num_labels = Y.shape[1]\n","\n","# Create the weights matrix\n","W = torch.tensor([[-0.1, 0.5, 0.3],\n","                  [0.9, 0.3, 0.5],\n","                  [-1.5, 0.4, 0.1],\n","                  [0.1, 0.1, -1.0],\n","                  [-1.2, 0.5, -0.8]], dtype = torch.float32)\n","\n","print(X_S)\n","print(Y)\n","print(W)"]},{"cell_type":"markdown","metadata":{"id":"1cQDyu7llDo9"},"source":["---\n","\n","Bias trick to absorb the bias into the weights matrix\n","\n","![bias trick](https://1drv.ms/i/c/37720f927b6ddc34/IQR8NDbhvaddQa3W3F_46q4nATD7WBNgnwGJ7QC6HDL6g14?width=850)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlviiS0tlH7p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974230166,"user_tz":-330,"elapsed":605,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"b45d8fa2-ec1f-469d-f201-df6a34b25290"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920,  1.0000],\n","        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374,  1.0000],\n","        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647,  1.0000],\n","        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439,  1.0000],\n","        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043,  1.0000],\n","        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676,  1.0000]])\n","tensor([[-0.1000,  0.5000,  0.3000],\n","        [ 0.9000,  0.3000,  0.5000],\n","        [-1.5000,  0.4000,  0.1000],\n","        [ 0.1000,  0.1000, -1.0000],\n","        [-1.2000,  0.5000, -0.8000],\n","        [ 0.1000,  0.1000,  0.1000]])\n"]}],"source":["## Bias trick to absorb the bias into the weights matrix\n","# Concatenate a column of ones to X_S (bias term)\n","X_B = torch.cat([X_S, torch.ones((num_samples, 1))], dim = 1)\n","\n","# Create the bias vector `b`\n","b = 0.1 * torch.ones((1, num_labels))\n","\n","# Concatenate the weights matrix `W` with the bias vector `b`\n","W_B = torch.cat([W, b], dim = 0)\n","\n","print(X_B)\n","print(W_B)"]},{"cell_type":"markdown","metadata":{"id":"5rZkNr8d1gAw"},"source":["---\n","\n","Forward propagation for the toy patient dataset: $$\\textbf{bias-added input }\\mathbf{X}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{raw scores }\\mathbf{Z}=\\mathbf{X}_B\\textbf{W}_B\\,{\\color{yellow}\\longrightarrow}\\,\\textbf{softmax activated scores }\\mathbf{A}=\\text{softmax}(\\mathbf{Z}).$$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5g-D7NLlZBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974329618,"user_tz":-330,"elapsed":680,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"bfb6e001-917a-4527-c72d-d394caf29426"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.5171, -0.5427, -0.3438],\n","        [ 3.6357, -0.6126,  1.9614],\n","        [-4.6127, -0.0660, -2.2940],\n","        [ 0.3821,  1.5427,  0.4789],\n","        [-1.5298,  1.4386, -1.5126],\n","        [ 3.2418, -1.1601,  2.3101]])\n","tensor([[0.3161, 0.3081, 0.3759],\n","        [0.8321, 0.0119, 0.1560],\n","        [0.0095, 0.8942, 0.0963],\n","        [0.1889, 0.6030, 0.2081],\n","        [0.0466, 0.9061, 0.0474],\n","        [0.7112, 0.0087, 0.2801]])\n","tensor([[0., 1., 0.],\n","        [1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.],\n","        [1., 0., 0.],\n","        [0., 0., 1.]])\n"]}],"source":["# Raw scores matrix\n","Z = torch.matmul(X_B, W_B) # also referred to as logits\n","print(Z)\n","\n","# Softmax activated scores\n","A = torch.nn.functional.softmax(Z, dim = 1)\n","\n","# Predicted probabilities for each sample\n","print(A)\n","\n","# True output label for each sample\n","print(Y)"]},{"cell_type":"markdown","metadata":{"id":"dgtOD11HljXv"},"source":["---\n","\n","Loss for each sample can be quantified using the categorical crossentropy (CCE) loss function which is defined as $$\\color{yellow}{-\\log(\\text{predicted probability that a sample belongs its correct class})}$$\n","\n","For example, consider a sample with\n","\n","- true_label = [$\\color{yellow}{1}$ 0 0]\n","- predicted_label = [$\\color{yellow}{0.05}$, 0.99, 0.05]\n","\n","categorical crossentropy loss = $-\\log(\\color{yellow}{0.05}).$\n","\n","Here, we calculate the average CCE loss for all all samples and average them out.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiCY-GvwlpDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735974704270,"user_tz":-330,"elapsed":615,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"541b6cb4-d4b1-4e41-eb6f-c757e1db3071"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.2304)\n","tensor(1.2304)\n"]}],"source":["## Calculate average CCE loss\n","loss = torch.mean(-torch.log(torch.sum(Y * A, dim = 1)))\n","print(loss)\n","\n","# Using the PyTorch in-built function for CCE loss\n","loss_fn = torch.nn.CrossEntropyLoss()\n","loss = loss_fn(Z, torch.argmax(Y, dim = 1)) # Note that the arguemnts are logits and correct labels\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"Iro8wo7tnv2g"},"source":["---\n","\n","Applying the gradient descent method with\n","\n","- a maximum number of iterations equal to 1000\n","- a stopping tolerance equal to $10^{-6}$\n","- a learning rate of 0.01\n","\n"," to minimize $$L(\\mathbf{w}) = (w_1-2)^2+(w_2+3)^2$$ starting from $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qcM3YnJmnwrY"},"outputs":[],"source":["# Initialize weights as tensors with gradients\n","w = torch.tensor([0.0, 0.0], requires_grad = True)\n","\n","# Hyperparameters\n","maxiter = 1000\n","tol = 1e-06\n","lr = 1e-02\n","norm_grad = float('inf')\n","\n","k = 0\n","while k < maxiter and norm_grad > tol:\n","    # Zero the gradients\n","    if w.grad is not None:\n","        w.grad.zero_()\n","\n","    # Define the loss function\n","    L = (w[0] - 2)**2 + (w[1] + 3)**2\n","\n","    # Backpropagate to compute gradients\n","    L.backward()\n","\n","    # Update weights using gradient descent\n","    with torch.no_grad():\n","        w -= lr * w.grad\n","\n","    # Compute the norm of the gradient\n","    norm_grad = w.grad.norm().item()\n","    k += 1\n","\n","    print(f'Iteration {k}: ||grad|| = {norm_grad}')"]},{"cell_type":"markdown","metadata":{"id":"IvRw58l8p2T3"},"source":["---\n","\n","We will consider again the same toy data matrix with 6 samples and 3 possible output labels :\n","\n","\n","![data for softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=960)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1ylibPssqTWs"},"source":["---\n","\n","Define the linear layer (dense layer) where the raw scores are calculated through the linear operation:\n","$$\\underbrace{\\mathbf{Z}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{z}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{z}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}+{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}} = \\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\mathbf{W}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\mathbf{W}\\end{bmatrix}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}=\\underbrace{\\begin{bmatrix}{\\mathbf{x}^{(0)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(1)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(2)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(3)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(4)}}^\\mathrm{T}\\\\{\\mathbf{x}^{(5)}}^\\mathrm{T}\\end{bmatrix}}_{\\color{red}{6\\times5}}\\underbrace{\\mathbf{W}}_{\\color{red}{5\\times3}}=\\underbrace{\\underbrace{\\mathbf{X}}_{6\\times 5}\\underbrace{\\mathbf{W}}_{5\\times 3}}_{\\color{red}{6\\times3}} + \\underbrace{\\begin{bmatrix}{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\\\{\\color{orange}{\\mathbf{b}}^\\mathrm{T}}\\end{bmatrix}}_{\\color{red}{6\\times3}}.$$\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vt14T0bgqUxF"},"outputs":[],"source":["class LinearLayer(torch.nn.Module):\n","    def __init__(self, input_dim, nodes = 2):\n","        super(LinearLayer, self).__init__()  # Initialize the parent class (nn.Module)\n","        self.nodes = nodes\n","        # Define the weights and bias as parameters\n","        self.W = torch.nn.Parameter(torch.Tensor(input_dim, self.nodes))\n","        torch.nn.init.xavier_uniform_(self.W)  # Xavier uniform initialization\n","        self.b = torch.nn.Parameter(torch.randn(self.nodes))  # Random Normal initialization\n","\n","    def forward(self, input):\n","        # Linear transformation (input * W + b)\n","        output = torch.matmul(input, self.W) + self.b\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"4Sb0TWaWqosF"},"source":["---\n","\n","Defining a LinearLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNrvxsYgqpve"},"outputs":[],"source":["layer1 = LinearLayer(num_features, 3)\n","print(layer1.W)\n","print(layer1.b)\n","layer1.forward(X_S)"]},{"cell_type":"markdown","metadata":{"id":"4P8i-XsrroCB"},"source":["---\n","\n","Define the softmax layer\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoACMi5FrokZ"},"outputs":[],"source":["class SoftmaxLayer(torch.nn.Module):\n","    def __init__(self):\n","        super(SoftmaxLayer, self).__init__()\n","        self.activation = torch.nn.Softmax(dim = 1)\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"wiSxk97fr4xJ"},"source":["---\n","\n","Defining a SoftmaxLayer object and calling the forward() method applied to the toy patient data matrix.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pn_dXu7xr7HV"},"outputs":[],"source":["actlayer1 = SoftmaxLayer()\n","print(actlayer1.activation)\n","actlayer1.forward(layer1.forward(X_S))"]},{"cell_type":"markdown","metadata":{"id":"JRQ0SwBxs3WL"},"source":["---\n","\n","Define the softmax classifier model\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKicexaas31_"},"outputs":[],"source":["class SoftmaxClassifierModel(torch.nn.Module):\n","    def __init__(self, input_dim, nodes=2):\n","        super(SoftmaxClassifierModel, self).__init__()\n","        self.nodes = nodes\n","        self.linearLayer = LinearLayer(input_dim, self.nodes)  # Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer(input)  # Forward pass through the linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"upcq6QXttB8y"},"source":["---\n","\n","Perform forward propagation to the toy patient dataset using the SoftmaxClassifierModel built above.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQIET4VFtNSA"},"outputs":[],"source":["model = SoftmaxClassifierModel(num_features, 3)\n","print(model(X_S))"]},{"cell_type":"markdown","metadata":{"id":"V3G_qHiVtWvj"},"source":["---\n","\n","Define loss function (categorical crossentropy).\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-QsTVjRtYAh"},"outputs":[],"source":["def loss_fn(true_labels, predicted_probs):\n","  loss = torch.mean(-torch.log(torch.sum(true_labels * predicted_probs, dim = 1)))\n","  return(loss)"]},{"cell_type":"markdown","metadata":{"id":"_LpnJT06u5KV"},"source":["---\n","\n","Apply the softmax classifier model to the toy data set and calculate the loss.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ih-XPwbdu5my"},"outputs":[],"source":["## Apply the softmax classifier model to the toy data set and calculate the loss\n","# Instantiate the model object\n","model = SoftmaxClassifierModel(num_features, 3) # invokes the constructor and sets up the layers\n","\n","# Calculate average data loss\n","print(Y)\n","print(model(X_S))\n","loss_fn(Y, model(X_S))"]},{"cell_type":"markdown","metadata":{"id":"s4A6iMHOzAAO"},"source":["---\n","\n","Softmax classifier for the [MNIST](https://www.tensorflow.org/datasets/catalog/mnist) dataset\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6Wc0cWFzI7X"},"outputs":[],"source":["## Load MNIST data (note that shape of X_train and y_train)\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","print(X_train.shape)\n","print(y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cP47GmErzKzV"},"outputs":[],"source":["## Reshape X_train and X_test such that the samples are along the rows\n","X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n","X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n","print(X_train_reshaped.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z54AQD5rzO12"},"outputs":[],"source":["## Problem parameters\n","num_samples_train = X_train_reshaped.shape[0]\n","num_samples_test = X_test_reshaped.shape[0]\n","num_features = X_train_reshaped.shape[1]\n","num_labels = len(np.unique(y_train))\n","print(f'No. of training samples = {num_samples_train},\\\n"," No. of test samples = {num_samples_test}, \\\n"," no. of features = {num_features}, no. of labels = {num_labels}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeTfB5-xzSGi"},"outputs":[],"source":["## One-hot encode output labels using scikit-learn (observe the shape of Y_train)\n","ohe = OneHotEncoder(sparse_output=False)\n","Y_train = torch.tensor(ohe.fit_transform(y_train.reshape(-1, 1)), dtype = torch.float32)\n","Y_test = torch.tensor(ohe.transform(y_test.reshape(-1, 1)), dtype = torch.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCD_yzX1zUE0"},"outputs":[],"source":["## Min-max scale the images using scikit-learn\n","mms = MinMaxScaler()\n","X_train_reshaped_scaled = torch.tensor(mms.fit_transform(X_train_reshaped), dtype=torch.float32)\n","X_test_reshaped_scaled = torch.tensor(mms.transform(X_test_reshaped), dtype=torch.float32)"]},{"cell_type":"markdown","metadata":{"id":"2OpSKsccees6"},"source":["---\n","\n","Train the softmax classifier on the MNIST dataset\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00eKgH6ez7Ix"},"outputs":[],"source":["## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = SoftmaxClassifierModel(num_features, num_labels)\n","\n","# Gradient descent\n","maxiter = 250\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","#loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"]},{"cell_type":"markdown","metadata":{"id":"Yd8PGQKs15kB"},"source":["---\n","\n","Plot training and test loss in the same figure\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HL1069mE16U_"},"outputs":[],"source":["## Plot the training and test loss\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","ax.plot(loss_train, 'b', label = 'Train')\n","ax.plot(loss_test, 'r', label = 'Test')\n","ax.set_xlabel('Iteration')\n","ax.set_ylabel('Loss')\n","ax.legend();"]},{"cell_type":"markdown","metadata":{"id":"Eqxn_hXR2BM9"},"source":["---\n","\n","Assess model performance on test data\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVdiXg6q2B4I"},"outputs":[],"source":["## Assess model performance on test data\n","Yhat = model(X_test_reshaped_scaled)\n","\n","ypred = np.array(torch.argmax(Yhat, axis = 1)) # predicted labels for the test samples\n","ytrue = np.array(torch.argmax(Y_test, axis = 1)) # true labels for the test samples\n","print('Accuracy on test data = %3.2f'%(np.mean(ytrue == ypred)*100))\n","# Print confusion matrix\n","print(confusion_matrix(ytrue, ypred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11ubRM9p2W3A"},"outputs":[],"source":["## Plot a random test sample with its predicted label printed above the plot\n","test_index = np.random.choice(X_test.shape[0])\n","fig, ax = plt.subplots(1, 1, figsize = (2, 2))\n","print(f'Image classified as {ypred[test_index]}')\n","ax.imshow(tf.reshape(X_test_reshaped_scaled[test_index], [28, 28]).numpy(), cmap = 'gray');"]},{"cell_type":"markdown","metadata":{"id":"brjKnRYGbz6x"},"source":["---\n","\n","Define a nonlinear activation layer with ReLU activation\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKu3VuDGb41Y"},"outputs":[],"source":["class ReLULayer(torch.nn.Module):\n","    def __init__(self):\n","        super(ReLULayer, self).__init__()\n","        self.activation = torch.nn.ReLU()\n","\n","    def forward(self, input):\n","        output = self.activation(input)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"NrbyHyBXcRGj"},"source":["---\n","\n","Define a one hidden layer neural network model\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ByD0iWjcXQF"},"outputs":[],"source":["class OneLayerNeuralNetworkModel(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_nodes = 2, nodes = 2):\n","        super(OneLayerNeuralNetworkModel, self).__init__()\n","        self.hidden_nodes = hidden_nodes\n","        self.nodes = nodes\n","        self.linearLayer1 = LinearLayer(input_dim, self.hidden_nodes)  # 1st Linear layer\n","        self.actlayer1 = ReLULayer() # 1st activation layer (ReLU)\n","        self.linearLayer2 = LinearLayer(self.hidden_nodes, self.nodes)  # 2nd Linear layer\n","        self.softmaxLayer = SoftmaxLayer()  # Softmax activation layer\n","\n","    def forward(self, input):\n","        output = self.linearLayer1(input)  # Forward pass through the 1st linear layer\n","        output = self.actlayer1(output) # ReLU activation\n","        output = self.linearLayer2(output)  # Forward pass through the 2nd linear layer\n","        output = self.softmaxLayer(output)  # Apply softmax activation\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"r3XJ5qcjdPaE"},"source":["---\n","\n","Perform forward propagation to the toy patient dataset using the NeuralNetworkModel built above.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skQYdCkadfCF"},"outputs":[],"source":["model = OneLayerNeuralNetworkModel(5, 4, 3) # 4 nodes in hidden layer\n","print(model(X_S))"]},{"cell_type":"markdown","metadata":{"id":"Ie2z0taWekVQ"},"source":["---\n","\n","Train the 1-hidden layer neural network classifier on the MNIST dataset\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWbCrpERen2d"},"outputs":[],"source":["# This is an exercise## Train the softmax classifier on the MNIST dataset\n","# Initialize model\n","model = OneLayerNeuralNetworkModel(num_features, 4, num_labels)\n","\n","# Gradient descent\n","maxiter = 2000\n","lr = 1e-03\n","\n","# Define loss function (CrossEntropyLoss in PyTorch includes softmax)\n","#loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Define optimizer (RMSprop)\n","optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n","\n","# Lists to store training and test losses\n","loss_train = [None] * maxiter\n","loss_test = [None] * maxiter\n","\n","# Start training loop\n","for k in range(maxiter):\n","    model.train()  # Set model to training mode\n","\n","    # Forward pass: compute predicted probabilities\n","    Yhat = model(X_train_reshaped_scaled)  # predicted probabilities\n","\n","    # Compute training loss\n","    L_train = loss_fn(Y_train, Yhat)  # CrossEntropyLoss expects raw logits (no softmax needed)\n","\n","    # Append training and test loss values\n","    loss_train[k] = L_train.item()  # Convert to scalar\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation for testing\n","        Yhat_test = model(X_test_reshaped_scaled)  # predicted probabilities for test set\n","        L_test = loss_fn(Y_test, Yhat_test)  # Compute test loss\n","    loss_test[k] = L_test.item()  # Convert to scalar\n","\n","    # Print losses\n","    print(f'Iteration {k+1}, Training loss = {loss_train[k]}, Test loss = {loss_test[k]}')\n","\n","    # Backward pass: compute gradients\n","    optimizer.zero_grad()  # Zero the gradients before the backward pass\n","    L_train.backward()  # Backpropagate the gradients\n","\n","    # Update model parameters using optimizer\n","    optimizer.step()  # Perform one optimization step"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"colab-windows","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}