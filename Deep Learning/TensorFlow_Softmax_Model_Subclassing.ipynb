{"cells":[{"cell_type":"markdown","source":["---\n","\n","**Load essential libraries**\n","\n","---"],"metadata":{"id":"CrW3wGfQN2KV"},"id":"CrW3wGfQN2KV"},{"cell_type":"code","execution_count":76,"id":"valued-lodging","metadata":{"id":"valued-lodging","executionInfo":{"status":"ok","timestamp":1729227308936,"user_tz":-330,"elapsed":613,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use('dark_background')\n","%matplotlib inline\n","import tensorflow as tf\n","import sys\n","import h5py # this library is used to save weights of the model\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"markdown","source":["---\n","\n","Load MNIST data\n","\n","---"],"metadata":{"id":"UYEjExZ9XJ5Z"},"id":"UYEjExZ9XJ5Z"},{"cell_type":"code","source":["## Load MNIST data (note that shape of X_train and y_train)\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","print(X_train.shape)\n","print(y_train.shape)"],"metadata":{"id":"Eige5qzmXKoF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729227310961,"user_tz":-330,"elapsed":1471,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"f28c11d3-3bd6-4292-9118-6a367790229f"},"id":"Eige5qzmXKoF","execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(60000,)\n"]}]},{"cell_type":"code","source":["## Reshape X_train and X_test such that the samples are along the rows\n","X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2])  # Reshape to (num_samples, 784)\n","X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n","print(X_train_reshaped.shape)\n","print(X_test_reshaped.shape)"],"metadata":{"id":"ucifTdfmmppB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729227310962,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"33a504af-8f97-4264-f950-0c8a366f795d"},"id":"ucifTdfmmppB","execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n","(10000, 784)\n"]}]},{"cell_type":"code","source":["## Min-max scale the images using scikit-learn\n","mms = MinMaxScaler()\n","X_train_reshaped_scaled = mms.fit_transform(X_train_reshaped)\n","X_test_reshaped_scaled = mms.transform(X_test_reshaped)"],"metadata":{"id":"_BgkMBAakHGZ","executionInfo":{"status":"ok","timestamp":1729227313073,"user_tz":-330,"elapsed":2114,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"_BgkMBAakHGZ","execution_count":79,"outputs":[]},{"cell_type":"code","source":["## Problem parameters\n","num_samples_train = X_train_reshaped_scaled.shape[0]\n","num_samples_test = X_test_reshaped_scaled.shape[0]\n","num_features = X_train_reshaped_scaled.shape[1]\n","num_labels = 10\n","print(f'No. of training samples = {num_samples_train},\\\n"," No. of test samples = {num_samples_test}, \\\n"," no. of features = {num_features}, no. of labels = {num_labels}')"],"metadata":{"id":"mOITV11PmuR8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729227313073,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"159d61c6-b60a-41de-f3f9-99a1feb4932d"},"id":"mOITV11PmuR8","execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["No. of training samples = 60000, No. of test samples = 10000,  no. of features = 784, no. of labels = 10\n"]}]},{"cell_type":"code","source":["## One-hot encode output labels using scikit-learn (observe the shape of Y_train)\n","ohc = OneHotEncoder()\n","Y_train = ohc.fit_transform(y_train.reshape(-1, 1))\n","Y_test = ohc.transform(y_test.reshape(-1, 1))\n","Y_train.shape"],"metadata":{"id":"rwu-hI65mxY7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729227313074,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"d4447ea7-c358-4849-d33d-f93b29feb906"},"id":"rwu-hI65mxY7","execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 10)"]},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["---\n","\n","Define the linear layer\n","\n","---"],"metadata":{"id":"stzqySCOlwE-"},"id":"stzqySCOlwE-"},{"cell_type":"code","source":["class LinearLayer(tf.keras.layers.Layer):\n","  def __init__(self, nodes=2):\n","    super().__init__() # use parent class 'Layer' constructor\n","    self.nodes = nodes\n","\n","  def build(self, input_shape): # initialize the variables accosiated with LinearLayer\n","    self.W = self.add_weight(shape = (input_shape[-1], self.nodes), # feautres x neurons\n","                             dtype = tf.float32,\n","                             initializer = tf.initializers.RandomNormal() # initialize weights values randomly but close to 0\n","                             )\n","    self.b = self.add_weight(shape = (self.nodes, ),\n","                             dtype = tf.float32,\n","                             initializer = tf.initializers.RandomUniform() # initialize bias values randomly in range(-1,1)\n","                             )\n","  def call(self, input): # perform compuation based on input\n","    input = tf.cast(input, dtype=tf.float32)\n","    output = tf.linalg.matmul(input, self.W) + self.b\n","    return output"],"metadata":{"id":"kTHnjC9vlzTu","executionInfo":{"status":"ok","timestamp":1729227313074,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"kTHnjC9vlzTu","execution_count":82,"outputs":[]},{"cell_type":"code","source":["ll = LinearLayer(nodes=num_labels)\n","Z = ll(X_train_reshaped_scaled)"],"metadata":{"id":"Icwt0OLUnTTg","executionInfo":{"status":"ok","timestamp":1729227315019,"user_tz":-330,"elapsed":1948,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"Icwt0OLUnTTg","execution_count":83,"outputs":[]},{"cell_type":"code","source":["Z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGY0Hfl1w8dc","executionInfo":{"status":"ok","timestamp":1729227315020,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"4b434d86-b7f9-477e-fc57-cb8e7ad8004f"},"id":"yGY0Hfl1w8dc","execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(60000, 10), dtype=float32, numpy=\n","array([[-0.39046022, -0.14868215,  0.5208598 , ...,  0.35303608,\n","         0.35605833,  0.29801464],\n","       [-0.45713276, -0.10147268,  0.1565415 , ...,  0.0139293 ,\n","         1.012447  ,  0.5571058 ],\n","       [ 0.24170652,  0.27016598, -0.25384447, ...,  0.07900737,\n","         0.28944093, -0.31392932],\n","       ...,\n","       [-0.4450251 , -0.48989248,  0.12073756, ...,  0.21708348,\n","         0.12653969,  0.2863836 ],\n","       [-0.48921925,  0.11717599,  0.43056288, ...,  0.01365707,\n","         0.16201678, -0.04729123],\n","       [ 0.25010952, -0.18425307,  0.25421447, ..., -0.24857438,\n","         0.45948187, -0.16830093]], dtype=float32)>"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["---\n","\n","Define the softmax layer\n","\n","---"],"metadata":{"id":"ruz468ArvW0m"},"id":"ruz468ArvW0m"},{"cell_type":"code","source":["class SoftmaxLayer(tf.keras.layers.Layer):\n","  def __init__(self):\n","    super().__init__()\n","    self.activation = tf.keras.layers.Softmax()\n","\n","  def call(self, input):\n","    input = tf.cast(input, dtype=tf.float32)\n","    output = self.activation(input)\n","    return output"],"metadata":{"id":"8SdvaOntvZHk","executionInfo":{"status":"ok","timestamp":1729227315020,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"8SdvaOntvZHk","execution_count":85,"outputs":[]},{"cell_type":"code","source":["softmax = SoftmaxLayer()\n","A = softmax(Z)"],"metadata":{"id":"YRVfdsOCwi6F","executionInfo":{"status":"ok","timestamp":1729227315020,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"YRVfdsOCwi6F","execution_count":86,"outputs":[]},{"cell_type":"code","source":["A"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEJ-O488w_K8","executionInfo":{"status":"ok","timestamp":1729227315020,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"3d332814-1042-44d8-ed3d-bf4d0d71dc36"},"id":"NEJ-O488w_K8","execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(60000, 10), dtype=float32, numpy=\n","array([[0.06203166, 0.07899803, 0.1543102 , ..., 0.13046972, 0.13086462,\n","        0.12348501],\n","       [0.05677274, 0.08102164, 0.10487095, ..., 0.09093258, 0.24681425,\n","        0.15653737],\n","       [0.12935847, 0.13309282, 0.07880972, ..., 0.10993487, 0.13568306,\n","        0.0742139 ],\n","       ...,\n","       [0.06208424, 0.05936025, 0.10931733, ..., 0.12037367, 0.10995344,\n","        0.12901142],\n","       [0.06405674, 0.11746784, 0.16070196, ..., 0.10591592, 0.12285507,\n","        0.09965332],\n","       [0.12843479, 0.08318431, 0.12896308, ..., 0.07800224, 0.15834777,\n","        0.08452192]], dtype=float32)>"]},"metadata":{},"execution_count":87}]},{"cell_type":"markdown","source":["---\n","\n","Define the softmax classifier model\n","\n","---"],"metadata":{"id":"z2FtRKgKufax"},"id":"z2FtRKgKufax"},{"cell_type":"code","source":["class SoftmaxClassifierModel(tf.keras.Model):\n","  def __init__(self, nodes=2):\n","    super().__init__()\n","    self.nodes = nodes\n","    self.ll = LinearLayer(self.nodes)\n","    self.softmax = SoftmaxLayer()\n","\n","  def call(self, input):\n","    input = tf.cast(input, dtype=tf.float32)\n","    Z = self.ll(input)\n","    A = self.softmax(Z)\n","    return A"],"metadata":{"id":"yLQ0iXTRwMD7","executionInfo":{"status":"ok","timestamp":1729227315020,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"yLQ0iXTRwMD7","execution_count":88,"outputs":[]},{"cell_type":"code","source":["model = SoftmaxClassifierModel(nodes=num_labels)\n","Y_hat = model(X_train_reshaped_scaled)"],"metadata":{"id":"E3DxHEd93QVr","executionInfo":{"status":"ok","timestamp":1729227315588,"user_tz":-330,"elapsed":571,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"E3DxHEd93QVr","execution_count":89,"outputs":[]},{"cell_type":"code","source":["Y_hat.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlcpQxmz6Cfq","executionInfo":{"status":"ok","timestamp":1729227315588,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"995650f9-511a-4599-b81a-61ed849e7dfc"},"id":"zlcpQxmz6Cfq","execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([60000, 10])"]},"metadata":{},"execution_count":90}]},{"cell_type":"markdown","source":["---\n","\n","Define loss function\n","\n","---"],"metadata":{"id":"YuysAloWwMrk"},"id":"YuysAloWwMrk"},{"cell_type":"code","source":["def loss_fn(actual, predicted):\n","  loss = tf.keras.losses.categorical_crossentropy(actual.todense(), predicted) # method\n","  '''\n","  cce = tf.keras.losses.CategoricalCrossentropy() # it is a class\n","  loss = cce(actual.todense(), predicted)\n","  '''\n","  return tf.reduce_mean(loss)"],"metadata":{"id":"4ZQNe1o6wPf-","executionInfo":{"status":"ok","timestamp":1729227315588,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}}},"id":"4ZQNe1o6wPf-","execution_count":91,"outputs":[]},{"cell_type":"code","source":["loss_fn(Y_train ,Y_hat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmyxCtfH5t0T","executionInfo":{"status":"ok","timestamp":1729227319395,"user_tz":-330,"elapsed":568,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"a05a5141-eec9-4e08-a862-dd94fc4f49e3"},"id":"qmyxCtfH5t0T","execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=2.4337692>"]},"metadata":{},"execution_count":92}]},{"cell_type":"markdown","source":["---\n","\n","Train model\n","\n","---"],"metadata":{"id":"sviMtRdusEwg"},"id":"sviMtRdusEwg"},{"cell_type":"code","source":["## Train model\n","nodes = 10\n","model = SoftmaxClassifierModel(nodes)\n","\n","# Gradient descent\n","maxiter = 1000\n","tol = 1e-04\n","lr = 1e-03\n","norm_grad = np.inf\n","optimizer = tf.optimizers.RMSprop(learning_rate=lr)\n","# Empty list to store training loss every step\n","loss_train = []\n","k = 0\n","while k < maxiter and norm_grad > tol:\n","  with tf.GradientTape() as tape:\n","    Yhat = model(X_train_reshaped_scaled)\n","    L = loss_fn(Y_train, Yhat)\n","\n","  # Append training loss values\n","  loss_train.append(L)\n","  k = k+1\n","  print(f'Iteration {k}, Training loss = {L}')\n","  gradients = tape.gradient(L, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # w = w + lr*gradient"],"metadata":{"id":"JhAfBIk85uC3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729229151829,"user_tz":-330,"elapsed":861604,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"392a6ec6-2ddb-4eca-a418-d598008fb085"},"id":"JhAfBIk85uC3","execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, Training loss = 2.371873378753662\n","Iteration 2, Training loss = 2.211620569229126\n","Iteration 3, Training loss = 2.106198310852051\n","Iteration 4, Training loss = 2.0224967002868652\n","Iteration 5, Training loss = 1.9515858888626099\n","Iteration 6, Training loss = 1.8893156051635742\n","Iteration 7, Training loss = 1.833390474319458\n","Iteration 8, Training loss = 1.7823835611343384\n","Iteration 9, Training loss = 1.7353392839431763\n","Iteration 10, Training loss = 1.69158136844635\n","Iteration 11, Training loss = 1.6506110429763794\n","Iteration 12, Training loss = 1.6120476722717285\n","Iteration 13, Training loss = 1.5755934715270996\n","Iteration 14, Training loss = 1.541009545326233\n","Iteration 15, Training loss = 1.5081007480621338\n","Iteration 16, Training loss = 1.4767062664031982\n","Iteration 17, Training loss = 1.4466900825500488\n","Iteration 18, Training loss = 1.4179368019104004\n","Iteration 19, Training loss = 1.3903471231460571\n","Iteration 20, Training loss = 1.3638352155685425\n","Iteration 21, Training loss = 1.3383253812789917\n","Iteration 22, Training loss = 1.3137515783309937\n","Iteration 23, Training loss = 1.2900547981262207\n","Iteration 24, Training loss = 1.2671823501586914\n","Iteration 25, Training loss = 1.2450865507125854\n","Iteration 26, Training loss = 1.2237249612808228\n","Iteration 27, Training loss = 1.2030583620071411\n","Iteration 28, Training loss = 1.183051347732544\n","Iteration 29, Training loss = 1.1636711359024048\n","Iteration 30, Training loss = 1.1448878049850464\n","Iteration 31, Training loss = 1.1266735792160034\n","Iteration 32, Training loss = 1.1090028285980225\n","Iteration 33, Training loss = 1.0918515920639038\n","Iteration 34, Training loss = 1.0751975774765015\n","Iteration 35, Training loss = 1.059019923210144\n","Iteration 36, Training loss = 1.0432990789413452\n","Iteration 37, Training loss = 1.0280165672302246\n","Iteration 38, Training loss = 1.013155221939087\n","Iteration 39, Training loss = 0.9986984133720398\n","Iteration 40, Training loss = 0.984630823135376\n","Iteration 41, Training loss = 0.9709374904632568\n","Iteration 42, Training loss = 0.9576048254966736\n","Iteration 43, Training loss = 0.9446192979812622\n","Iteration 44, Training loss = 0.9319683313369751\n","Iteration 45, Training loss = 0.9196399450302124\n","Iteration 46, Training loss = 0.9076228141784668\n","Iteration 47, Training loss = 0.8959059715270996\n","Iteration 48, Training loss = 0.8844789862632751\n","Iteration 49, Training loss = 0.8733320236206055\n","Iteration 50, Training loss = 0.8624558448791504\n","Iteration 51, Training loss = 0.8518412709236145\n","Iteration 52, Training loss = 0.8414799571037292\n","Iteration 53, Training loss = 0.8313637375831604\n","Iteration 54, Training loss = 0.8214847445487976\n","Iteration 55, Training loss = 0.8118357062339783\n","Iteration 56, Training loss = 0.8024094104766846\n","Iteration 57, Training loss = 0.7931992411613464\n","Iteration 58, Training loss = 0.7841986417770386\n","Iteration 59, Training loss = 0.7754012942314148\n","Iteration 60, Training loss = 0.7668015360832214\n","Iteration 61, Training loss = 0.758393406867981\n","Iteration 62, Training loss = 0.7501716017723083\n","Iteration 63, Training loss = 0.7421307563781738\n","Iteration 64, Training loss = 0.7342657446861267\n","Iteration 65, Training loss = 0.726571798324585\n","Iteration 66, Training loss = 0.7190442681312561\n","Iteration 67, Training loss = 0.7116785049438477\n","Iteration 68, Training loss = 0.704470157623291\n","Iteration 69, Training loss = 0.6974152326583862\n","Iteration 70, Training loss = 0.6905094981193542\n","Iteration 71, Training loss = 0.6837491989135742\n","Iteration 72, Training loss = 0.6771304607391357\n","Iteration 73, Training loss = 0.6706495881080627\n","Iteration 74, Training loss = 0.6643033623695374\n","Iteration 75, Training loss = 0.6580880880355835\n","Iteration 76, Training loss = 0.6520006656646729\n","Iteration 77, Training loss = 0.6460378170013428\n","Iteration 78, Training loss = 0.6401965618133545\n","Iteration 79, Training loss = 0.634473979473114\n","Iteration 80, Training loss = 0.6288669109344482\n","Iteration 81, Training loss = 0.6233727931976318\n","Iteration 82, Training loss = 0.6179889440536499\n","Iteration 83, Training loss = 0.6127126216888428\n","Iteration 84, Training loss = 0.6075416207313538\n","Iteration 85, Training loss = 0.6024739742279053\n","Iteration 86, Training loss = 0.5975096225738525\n","Iteration 87, Training loss = 0.5926530957221985\n","Iteration 88, Training loss = 0.587918221950531\n","Iteration 89, Training loss = 0.5833244919776917\n","Iteration 90, Training loss = 0.578845202922821\n","Iteration 91, Training loss = 0.57443767786026\n","Iteration 92, Training loss = 0.5700507760047913\n","Iteration 93, Training loss = 0.5657350420951843\n","Iteration 94, Training loss = 0.5614928603172302\n","Iteration 95, Training loss = 0.5573517084121704\n","Iteration 96, Training loss = 0.5532960891723633\n","Iteration 97, Training loss = 0.5493270754814148\n","Iteration 98, Training loss = 0.545432984828949\n","Iteration 99, Training loss = 0.5416150093078613\n","Iteration 100, Training loss = 0.5378669500350952\n","Iteration 101, Training loss = 0.5341904163360596\n","Iteration 102, Training loss = 0.5305812358856201\n","Iteration 103, Training loss = 0.5270406603813171\n","Iteration 104, Training loss = 0.5235655307769775\n","Iteration 105, Training loss = 0.5201576948165894\n","Iteration 106, Training loss = 0.5168138742446899\n","Iteration 107, Training loss = 0.5135368704795837\n","Iteration 108, Training loss = 0.5103211402893066\n","Iteration 109, Training loss = 0.507169783115387\n","Iteration 110, Training loss = 0.5040722489356995\n","Iteration 111, Training loss = 0.5010336637496948\n","Iteration 112, Training loss = 0.498040109872818\n","Iteration 113, Training loss = 0.495101660490036\n","Iteration 114, Training loss = 0.4922054708003998\n","Iteration 115, Training loss = 0.48936405777931213\n","Iteration 116, Training loss = 0.486566424369812\n","Iteration 117, Training loss = 0.4838229715824127\n","Iteration 118, Training loss = 0.4811241328716278\n","Iteration 119, Training loss = 0.4784771502017975\n","Iteration 120, Training loss = 0.47587382793426514\n","Iteration 121, Training loss = 0.47331979870796204\n","Iteration 122, Training loss = 0.4708077907562256\n","Iteration 123, Training loss = 0.46834295988082886\n","Iteration 124, Training loss = 0.4659183919429779\n","Iteration 125, Training loss = 0.46353912353515625\n","Iteration 126, Training loss = 0.4611978530883789\n","Iteration 127, Training loss = 0.4589000344276428\n","Iteration 128, Training loss = 0.4566372334957123\n","Iteration 129, Training loss = 0.4544159471988678\n","Iteration 130, Training loss = 0.45222654938697815\n","Iteration 131, Training loss = 0.45007675886154175\n","Iteration 132, Training loss = 0.44795650243759155\n","Iteration 133, Training loss = 0.44587442278862\n","Iteration 134, Training loss = 0.4438205063343048\n","Iteration 135, Training loss = 0.4418037235736847\n","Iteration 136, Training loss = 0.43981459736824036\n","Iteration 137, Training loss = 0.4378615617752075\n","Iteration 138, Training loss = 0.4359358251094818\n","Iteration 139, Training loss = 0.4340450167655945\n","Iteration 140, Training loss = 0.43218082189559937\n","Iteration 141, Training loss = 0.4303501844406128\n","Iteration 142, Training loss = 0.4285450279712677\n","Iteration 143, Training loss = 0.42677199840545654\n","Iteration 144, Training loss = 0.42502322793006897\n","Iteration 145, Training loss = 0.4233051836490631\n","Iteration 146, Training loss = 0.4216097891330719\n","Iteration 147, Training loss = 0.4199439585208893\n","Iteration 148, Training loss = 0.41829928755760193\n","Iteration 149, Training loss = 0.4166830778121948\n","Iteration 150, Training loss = 0.4150867164134979\n","Iteration 151, Training loss = 0.4135179817676544\n","Iteration 152, Training loss = 0.41196802258491516\n","Iteration 153, Training loss = 0.41044485569000244\n","Iteration 154, Training loss = 0.40893980860710144\n","Iteration 155, Training loss = 0.4074607491493225\n","Iteration 156, Training loss = 0.4059993028640747\n","Iteration 157, Training loss = 0.4045630991458893\n","Iteration 158, Training loss = 0.40314391255378723\n","Iteration 159, Training loss = 0.4017491638660431\n","Iteration 160, Training loss = 0.4003707766532898\n","Iteration 161, Training loss = 0.39901602268218994\n","Iteration 162, Training loss = 0.39767688512802124\n","Iteration 163, Training loss = 0.3963604271411896\n","Iteration 164, Training loss = 0.395058810710907\n","Iteration 165, Training loss = 0.3937791585922241\n","Iteration 166, Training loss = 0.3925134837627411\n","Iteration 167, Training loss = 0.39126914739608765\n","Iteration 168, Training loss = 0.39003798365592957\n","Iteration 169, Training loss = 0.38882753252983093\n","Iteration 170, Training loss = 0.3876296579837799\n","Iteration 171, Training loss = 0.38645192980766296\n","Iteration 172, Training loss = 0.3852861225605011\n","Iteration 173, Training loss = 0.3841400146484375\n","Iteration 174, Training loss = 0.3830053508281708\n","Iteration 175, Training loss = 0.3818897604942322\n","Iteration 176, Training loss = 0.380785197019577\n","Iteration 177, Training loss = 0.37969914078712463\n","Iteration 178, Training loss = 0.37862369418144226\n","Iteration 179, Training loss = 0.37756621837615967\n","Iteration 180, Training loss = 0.3765188157558441\n","Iteration 181, Training loss = 0.3754887878894806\n","Iteration 182, Training loss = 0.37446847558021545\n","Iteration 183, Training loss = 0.37346503138542175\n","Iteration 184, Training loss = 0.37247076630592346\n","Iteration 185, Training loss = 0.37149298191070557\n","Iteration 186, Training loss = 0.3705238997936249\n","Iteration 187, Training loss = 0.369570791721344\n","Iteration 188, Training loss = 0.3686259984970093\n","Iteration 189, Training loss = 0.3676968216896057\n","Iteration 190, Training loss = 0.3667755126953125\n","Iteration 191, Training loss = 0.3658693730831146\n","Iteration 192, Training loss = 0.3649708330631256\n","Iteration 193, Training loss = 0.3640870451927185\n","Iteration 194, Training loss = 0.3632104694843292\n","Iteration 195, Training loss = 0.3623483180999756\n","Iteration 196, Training loss = 0.36149299144744873\n","Iteration 197, Training loss = 0.36065173149108887\n","Iteration 198, Training loss = 0.3598170280456543\n","Iteration 199, Training loss = 0.3589960038661957\n","Iteration 200, Training loss = 0.3581812381744385\n","Iteration 201, Training loss = 0.3573797941207886\n","Iteration 202, Training loss = 0.3565843105316162\n","Iteration 203, Training loss = 0.3558018207550049\n","Iteration 204, Training loss = 0.355025053024292\n","Iteration 205, Training loss = 0.35426095128059387\n","Iteration 206, Training loss = 0.35350221395492554\n","Iteration 207, Training loss = 0.3527558445930481\n","Iteration 208, Training loss = 0.3520146608352661\n","Iteration 209, Training loss = 0.35128548741340637\n","Iteration 210, Training loss = 0.350561261177063\n","Iteration 211, Training loss = 0.34984880685806274\n","Iteration 212, Training loss = 0.3491409718990326\n","Iteration 213, Training loss = 0.3484446704387665\n","Iteration 214, Training loss = 0.34775280952453613\n","Iteration 215, Training loss = 0.34707212448120117\n","Iteration 216, Training loss = 0.3463957607746124\n","Iteration 217, Training loss = 0.34573033452033997\n","Iteration 218, Training loss = 0.34506890177726746\n","Iteration 219, Training loss = 0.3444181978702545\n","Iteration 220, Training loss = 0.343771368265152\n","Iteration 221, Training loss = 0.3431349992752075\n","Iteration 222, Training loss = 0.34250226616859436\n","Iteration 223, Training loss = 0.34187981486320496\n","Iteration 224, Training loss = 0.3412608206272125\n","Iteration 225, Training loss = 0.34065181016921997\n","Iteration 226, Training loss = 0.34004610776901245\n","Iteration 227, Training loss = 0.3394501805305481\n","Iteration 228, Training loss = 0.33885741233825684\n","Iteration 229, Training loss = 0.338274210691452\n","Iteration 230, Training loss = 0.3376939594745636\n","Iteration 231, Training loss = 0.3371231257915497\n","Iteration 232, Training loss = 0.33655500411987305\n","Iteration 233, Training loss = 0.335996150970459\n","Iteration 234, Training loss = 0.33543992042541504\n","Iteration 235, Training loss = 0.33489271998405457\n","Iteration 236, Training loss = 0.33434799313545227\n","Iteration 237, Training loss = 0.33381205797195435\n","Iteration 238, Training loss = 0.33327850699424744\n","Iteration 239, Training loss = 0.33275365829467773\n","Iteration 240, Training loss = 0.33223098516464233\n","Iteration 241, Training loss = 0.3317168056964874\n","Iteration 242, Training loss = 0.33120471239089966\n","Iteration 243, Training loss = 0.33070096373558044\n","Iteration 244, Training loss = 0.33019915223121643\n","Iteration 245, Training loss = 0.32970550656318665\n","Iteration 246, Training loss = 0.3292137384414673\n","Iteration 247, Training loss = 0.32872986793518066\n","Iteration 248, Training loss = 0.3282478153705597\n","Iteration 249, Training loss = 0.3277735710144043\n","Iteration 250, Training loss = 0.3273009657859802\n","Iteration 251, Training loss = 0.3268360495567322\n","Iteration 252, Training loss = 0.3263726234436035\n","Iteration 253, Training loss = 0.3259167969226837\n","Iteration 254, Training loss = 0.32546237111091614\n","Iteration 255, Training loss = 0.3250153362751007\n","Iteration 256, Training loss = 0.3245696723461151\n","Iteration 257, Training loss = 0.32413119077682495\n","Iteration 258, Training loss = 0.32369402050971985\n","Iteration 259, Training loss = 0.323263943195343\n","Iteration 260, Training loss = 0.3228350281715393\n","Iteration 261, Training loss = 0.3224131166934967\n","Iteration 262, Training loss = 0.32199227809906006\n","Iteration 263, Training loss = 0.32157832384109497\n","Iteration 264, Training loss = 0.3211653530597687\n","Iteration 265, Training loss = 0.320759117603302\n","Iteration 266, Training loss = 0.32035377621650696\n","Iteration 267, Training loss = 0.31995508074760437\n","Iteration 268, Training loss = 0.31955718994140625\n","Iteration 269, Training loss = 0.3191658556461334\n","Iteration 270, Training loss = 0.3187752664089203\n","Iteration 271, Training loss = 0.3183910548686981\n","Iteration 272, Training loss = 0.31800755858421326\n","Iteration 273, Training loss = 0.3176303505897522\n","Iteration 274, Training loss = 0.3172537386417389\n","Iteration 275, Training loss = 0.3168833553791046\n","Iteration 276, Training loss = 0.3165134787559509\n","Iteration 277, Training loss = 0.3161497414112091\n","Iteration 278, Training loss = 0.3157864809036255\n","Iteration 279, Training loss = 0.3154292106628418\n","Iteration 280, Training loss = 0.31507232785224915\n","Iteration 281, Training loss = 0.31472134590148926\n","Iteration 282, Training loss = 0.3143707811832428\n","Iteration 283, Training loss = 0.31402596831321716\n","Iteration 284, Training loss = 0.3136815130710602\n","Iteration 285, Training loss = 0.31334272027015686\n","Iteration 286, Training loss = 0.31300419569015503\n","Iteration 287, Training loss = 0.3126713037490845\n","Iteration 288, Training loss = 0.31233856081962585\n","Iteration 289, Training loss = 0.31201139092445374\n","Iteration 290, Training loss = 0.31168437004089355\n","Iteration 291, Training loss = 0.3113628029823303\n","Iteration 292, Training loss = 0.31104129552841187\n","Iteration 293, Training loss = 0.3107251822948456\n","Iteration 294, Training loss = 0.3104091286659241\n","Iteration 295, Training loss = 0.3100983500480652\n","Iteration 296, Training loss = 0.3097875714302063\n","Iteration 297, Training loss = 0.30948197841644287\n","Iteration 298, Training loss = 0.30917638540267944\n","Iteration 299, Training loss = 0.3088759183883667\n","Iteration 300, Training loss = 0.3085753917694092\n","Iteration 301, Training loss = 0.3082798719406128\n","Iteration 302, Training loss = 0.30798426270484924\n","Iteration 303, Training loss = 0.30769360065460205\n","Iteration 304, Training loss = 0.3074028789997101\n","Iteration 305, Training loss = 0.30711692571640015\n","Iteration 306, Training loss = 0.30683085322380066\n","Iteration 307, Training loss = 0.306549608707428\n","Iteration 308, Training loss = 0.3062681555747986\n","Iteration 309, Training loss = 0.30599144101142883\n","Iteration 310, Training loss = 0.3057144582271576\n","Iteration 311, Training loss = 0.3054421842098236\n","Iteration 312, Training loss = 0.3051696717739105\n","Iteration 313, Training loss = 0.30490168929100037\n","Iteration 314, Training loss = 0.3046334683895111\n","Iteration 315, Training loss = 0.30436971783638\n","Iteration 316, Training loss = 0.3041056990623474\n","Iteration 317, Training loss = 0.303846150636673\n","Iteration 318, Training loss = 0.30358627438545227\n","Iteration 319, Training loss = 0.30333080887794495\n","Iteration 320, Training loss = 0.3030748963356018\n","Iteration 321, Training loss = 0.30282333493232727\n","Iteration 322, Training loss = 0.3025714159011841\n","Iteration 323, Training loss = 0.3023237884044647\n","Iteration 324, Training loss = 0.30207571387290955\n","Iteration 325, Training loss = 0.3018318712711334\n","Iteration 326, Training loss = 0.3015875518321991\n","Iteration 327, Training loss = 0.30134743452072144\n","Iteration 328, Training loss = 0.30110684037208557\n","Iteration 329, Training loss = 0.3008703589439392\n","Iteration 330, Training loss = 0.30063337087631226\n","Iteration 331, Training loss = 0.3004004657268524\n","Iteration 332, Training loss = 0.3001669943332672\n","Iteration 333, Training loss = 0.29993757605552673\n","Iteration 334, Training loss = 0.2997075617313385\n","Iteration 335, Training loss = 0.2994815409183502\n","Iteration 336, Training loss = 0.2992549538612366\n","Iteration 337, Training loss = 0.2990323007106781\n","Iteration 338, Training loss = 0.2988090217113495\n","Iteration 339, Training loss = 0.29858964681625366\n","Iteration 340, Training loss = 0.2983695864677429\n","Iteration 341, Training loss = 0.2981533706188202\n","Iteration 342, Training loss = 0.2979365885257721\n","Iteration 343, Training loss = 0.2977235019207001\n","Iteration 344, Training loss = 0.2975097894668579\n","Iteration 345, Training loss = 0.2972998023033142\n","Iteration 346, Training loss = 0.29708918929100037\n","Iteration 347, Training loss = 0.2968822121620178\n","Iteration 348, Training loss = 0.29667454957962036\n","Iteration 349, Training loss = 0.2964705526828766\n","Iteration 350, Training loss = 0.29626578092575073\n","Iteration 351, Training loss = 0.2960646450519562\n","Iteration 352, Training loss = 0.2958628237247467\n","Iteration 353, Training loss = 0.295664519071579\n","Iteration 354, Training loss = 0.29546549916267395\n","Iteration 355, Training loss = 0.29526999592781067\n","Iteration 356, Training loss = 0.2950737178325653\n","Iteration 357, Training loss = 0.2948809266090393\n","Iteration 358, Training loss = 0.2946873605251312\n","Iteration 359, Training loss = 0.29449719190597534\n","Iteration 360, Training loss = 0.29430627822875977\n","Iteration 361, Training loss = 0.29411882162094116\n","Iteration 362, Training loss = 0.2939305007457733\n","Iteration 363, Training loss = 0.2937455177307129\n","Iteration 364, Training loss = 0.2935597598552704\n","Iteration 365, Training loss = 0.2933773100376129\n","Iteration 366, Training loss = 0.29319408535957336\n","Iteration 367, Training loss = 0.2930140495300293\n","Iteration 368, Training loss = 0.29283326864242554\n","Iteration 369, Training loss = 0.29265573620796204\n","Iteration 370, Training loss = 0.2924772799015045\n","Iteration 371, Training loss = 0.29230207204818726\n","Iteration 372, Training loss = 0.29212602972984314\n","Iteration 373, Training loss = 0.2919531762599945\n","Iteration 374, Training loss = 0.29177942872047424\n","Iteration 375, Training loss = 0.2916088104248047\n","Iteration 376, Training loss = 0.2914373576641083\n","Iteration 377, Training loss = 0.2912690043449402\n","Iteration 378, Training loss = 0.29109978675842285\n","Iteration 379, Training loss = 0.2909335792064667\n","Iteration 380, Training loss = 0.29076653718948364\n","Iteration 381, Training loss = 0.29060253500938416\n","Iteration 382, Training loss = 0.29043757915496826\n","Iteration 383, Training loss = 0.2902757227420807\n","Iteration 384, Training loss = 0.29011285305023193\n","Iteration 385, Training loss = 0.2899530529975891\n","Iteration 386, Training loss = 0.2897922992706299\n","Iteration 387, Training loss = 0.28963443636894226\n","Iteration 388, Training loss = 0.2894757091999054\n","Iteration 389, Training loss = 0.2893199324607849\n","Iteration 390, Training loss = 0.289163202047348\n","Iteration 391, Training loss = 0.28900930285453796\n","Iteration 392, Training loss = 0.28885453939437866\n","Iteration 393, Training loss = 0.2887026071548462\n","Iteration 394, Training loss = 0.2885497510433197\n","Iteration 395, Training loss = 0.28839966654777527\n","Iteration 396, Training loss = 0.2882486581802368\n","Iteration 397, Training loss = 0.2881004810333252\n","Iteration 398, Training loss = 0.2879512906074524\n","Iteration 399, Training loss = 0.2878049612045288\n","Iteration 400, Training loss = 0.28765758872032166\n","Iteration 401, Training loss = 0.28751301765441895\n","Iteration 402, Training loss = 0.28736743330955505\n","Iteration 403, Training loss = 0.2872246205806732\n","Iteration 404, Training loss = 0.2870807945728302\n","Iteration 405, Training loss = 0.28693968057632446\n","Iteration 406, Training loss = 0.28679755330085754\n","Iteration 407, Training loss = 0.2866581380367279\n","Iteration 408, Training loss = 0.2865177094936371\n","Iteration 409, Training loss = 0.28637999296188354\n","Iteration 410, Training loss = 0.28624120354652405\n","Iteration 411, Training loss = 0.28610506653785706\n","Iteration 412, Training loss = 0.28596794605255127\n","Iteration 413, Training loss = 0.2858333885669708\n","Iteration 414, Training loss = 0.2856978476047516\n","Iteration 415, Training loss = 0.2855648994445801\n","Iteration 416, Training loss = 0.2854309380054474\n","Iteration 417, Training loss = 0.28529953956604004\n","Iteration 418, Training loss = 0.2851671278476715\n","Iteration 419, Training loss = 0.28503724932670593\n","Iteration 420, Training loss = 0.2849063277244568\n","Iteration 421, Training loss = 0.2847779393196106\n","Iteration 422, Training loss = 0.28464850783348083\n","Iteration 423, Training loss = 0.28452157974243164\n","Iteration 424, Training loss = 0.2843936085700989\n","Iteration 425, Training loss = 0.2842681407928467\n","Iteration 426, Training loss = 0.2841416001319885\n","Iteration 427, Training loss = 0.2840175926685333\n","Iteration 428, Training loss = 0.2838924527168274\n","Iteration 429, Training loss = 0.28376978635787964\n","Iteration 430, Training loss = 0.2836461067199707\n","Iteration 431, Training loss = 0.2835248112678528\n","Iteration 432, Training loss = 0.2834024131298065\n","Iteration 433, Training loss = 0.28328248858451843\n","Iteration 434, Training loss = 0.2831614911556244\n","Iteration 435, Training loss = 0.283042848110199\n","Iteration 436, Training loss = 0.2829231917858124\n","Iteration 437, Training loss = 0.282805860042572\n","Iteration 438, Training loss = 0.2826874256134033\n","Iteration 439, Training loss = 0.282571405172348\n","Iteration 440, Training loss = 0.28245434165000916\n","Iteration 441, Training loss = 0.28233951330184937\n","Iteration 442, Training loss = 0.2822236716747284\n","Iteration 443, Training loss = 0.28211015462875366\n","Iteration 444, Training loss = 0.2819955348968506\n","Iteration 445, Training loss = 0.28188320994377136\n","Iteration 446, Training loss = 0.2817697823047638\n","Iteration 447, Training loss = 0.28165867924690247\n","Iteration 448, Training loss = 0.2815464437007904\n","Iteration 449, Training loss = 0.2814365327358246\n","Iteration 450, Training loss = 0.28132548928260803\n","Iteration 451, Training loss = 0.28121668100357056\n","Iteration 452, Training loss = 0.2811068296432495\n","Iteration 453, Training loss = 0.28099915385246277\n","Iteration 454, Training loss = 0.28089043498039246\n","Iteration 455, Training loss = 0.28078392148017883\n","Iteration 456, Training loss = 0.28067630529403687\n","Iteration 457, Training loss = 0.2805708348751068\n","Iteration 458, Training loss = 0.2804643511772156\n","Iteration 459, Training loss = 0.28036001324653625\n","Iteration 460, Training loss = 0.2802545428276062\n","Iteration 461, Training loss = 0.2801513075828552\n","Iteration 462, Training loss = 0.2800469398498535\n","Iteration 463, Training loss = 0.2799447178840637\n","Iteration 464, Training loss = 0.2798413932323456\n","Iteration 465, Training loss = 0.27974021434783936\n","Iteration 466, Training loss = 0.2796379327774048\n","Iteration 467, Training loss = 0.27953776717185974\n","Iteration 468, Training loss = 0.27943649888038635\n","Iteration 469, Training loss = 0.2793373167514801\n","Iteration 470, Training loss = 0.2792370319366455\n","Iteration 471, Training loss = 0.27913886308670044\n","Iteration 472, Training loss = 0.279039591550827\n","Iteration 473, Training loss = 0.278942346572876\n","Iteration 474, Training loss = 0.27884402871131897\n","Iteration 475, Training loss = 0.2787477970123291\n","Iteration 476, Training loss = 0.2786504328250885\n","Iteration 477, Training loss = 0.27855512499809265\n","Iteration 478, Training loss = 0.27845868468284607\n","Iteration 479, Training loss = 0.27836430072784424\n","Iteration 480, Training loss = 0.27826881408691406\n","Iteration 481, Training loss = 0.2781752943992615\n","Iteration 482, Training loss = 0.2780807316303253\n","Iteration 483, Training loss = 0.27798810601234436\n","Iteration 484, Training loss = 0.27789443731307983\n","Iteration 485, Training loss = 0.2778027057647705\n","Iteration 486, Training loss = 0.2777099311351776\n","Iteration 487, Training loss = 0.27761906385421753\n","Iteration 488, Training loss = 0.2775271534919739\n","Iteration 489, Training loss = 0.2774371802806854\n","Iteration 490, Training loss = 0.27734607458114624\n","Iteration 491, Training loss = 0.27725693583488464\n","Iteration 492, Training loss = 0.2771666646003723\n","Iteration 493, Training loss = 0.27707839012145996\n","Iteration 494, Training loss = 0.2769889533519745\n","Iteration 495, Training loss = 0.2769014537334442\n","Iteration 496, Training loss = 0.276812881231308\n","Iteration 497, Training loss = 0.2767261862754822\n","Iteration 498, Training loss = 0.2766384184360504\n","Iteration 499, Training loss = 0.2765524983406067\n","Iteration 500, Training loss = 0.276465505361557\n","Iteration 501, Training loss = 0.27638038992881775\n","Iteration 502, Training loss = 0.27629417181015015\n","Iteration 503, Training loss = 0.27620983123779297\n","Iteration 504, Training loss = 0.27612435817718506\n","Iteration 505, Training loss = 0.2760407626628876\n","Iteration 506, Training loss = 0.27595606446266174\n","Iteration 507, Training loss = 0.27587324380874634\n","Iteration 508, Training loss = 0.2757892906665802\n","Iteration 509, Training loss = 0.2757071554660797\n","Iteration 510, Training loss = 0.27562394738197327\n","Iteration 511, Training loss = 0.27554258704185486\n","Iteration 512, Training loss = 0.2754600942134857\n","Iteration 513, Training loss = 0.2753794193267822\n","Iteration 514, Training loss = 0.2752976417541504\n","Iteration 515, Training loss = 0.2752177119255066\n","Iteration 516, Training loss = 0.2751366198062897\n","Iteration 517, Training loss = 0.275057315826416\n","Iteration 518, Training loss = 0.2749769389629364\n","Iteration 519, Training loss = 0.27489838004112244\n","Iteration 520, Training loss = 0.27481865882873535\n","Iteration 521, Training loss = 0.2747407555580139\n","Iteration 522, Training loss = 0.27466171979904175\n","Iteration 523, Training loss = 0.27458447217941284\n","Iteration 524, Training loss = 0.2745061218738556\n","Iteration 525, Training loss = 0.2744295001029968\n","Iteration 526, Training loss = 0.2743518352508545\n","Iteration 527, Training loss = 0.27427583932876587\n","Iteration 528, Training loss = 0.2741988003253937\n","Iteration 529, Training loss = 0.2741234600543976\n","Iteration 530, Training loss = 0.27404701709747314\n","Iteration 531, Training loss = 0.27397236227989197\n","Iteration 532, Training loss = 0.27389654517173767\n","Iteration 533, Training loss = 0.27382248640060425\n","Iteration 534, Training loss = 0.2737472653388977\n","Iteration 535, Training loss = 0.2736738324165344\n","Iteration 536, Training loss = 0.27359920740127563\n","Iteration 537, Training loss = 0.2735263705253601\n","Iteration 538, Training loss = 0.27345237135887146\n","Iteration 539, Training loss = 0.2733800709247589\n","Iteration 540, Training loss = 0.2733067274093628\n","Iteration 541, Training loss = 0.273235023021698\n","Iteration 542, Training loss = 0.27316224575042725\n","Iteration 543, Training loss = 0.2730911076068878\n","Iteration 544, Training loss = 0.27301886677742004\n","Iteration 545, Training loss = 0.27294832468032837\n","Iteration 546, Training loss = 0.27287665009498596\n","Iteration 547, Training loss = 0.27280667424201965\n","Iteration 548, Training loss = 0.2727355659008026\n","Iteration 549, Training loss = 0.2726661264896393\n","Iteration 550, Training loss = 0.2725955843925476\n","Iteration 551, Training loss = 0.27252665162086487\n","Iteration 552, Training loss = 0.27245667576789856\n","Iteration 553, Training loss = 0.2723883092403412\n","Iteration 554, Training loss = 0.27231884002685547\n","Iteration 555, Training loss = 0.2722510099411011\n","Iteration 556, Training loss = 0.27218207716941833\n","Iteration 557, Training loss = 0.2721147835254669\n","Iteration 558, Training loss = 0.2720463275909424\n","Iteration 559, Training loss = 0.27197954058647156\n","Iteration 560, Training loss = 0.2719116508960724\n","Iteration 561, Training loss = 0.27184537053108215\n","Iteration 562, Training loss = 0.2717779874801636\n","Iteration 563, Training loss = 0.27171218395233154\n","Iteration 564, Training loss = 0.27164527773857117\n","Iteration 565, Training loss = 0.2715800106525421\n","Iteration 566, Training loss = 0.27151361107826233\n","Iteration 567, Training loss = 0.2714488208293915\n","Iteration 568, Training loss = 0.2713828682899475\n","Iteration 569, Training loss = 0.27131858468055725\n","Iteration 570, Training loss = 0.27125316858291626\n","Iteration 571, Training loss = 0.27118930220603943\n","Iteration 572, Training loss = 0.27112433314323425\n","Iteration 573, Training loss = 0.271060973405838\n","Iteration 574, Training loss = 0.27099648118019104\n","Iteration 575, Training loss = 0.2709335684776306\n","Iteration 576, Training loss = 0.27086955308914185\n","Iteration 577, Training loss = 0.27080708742141724\n","Iteration 578, Training loss = 0.27074354887008667\n","Iteration 579, Training loss = 0.2706815302371979\n","Iteration 580, Training loss = 0.27061840891838074\n","Iteration 581, Training loss = 0.27055683732032776\n","Iteration 582, Training loss = 0.27049416303634644\n","Iteration 583, Training loss = 0.27043306827545166\n","Iteration 584, Training loss = 0.27037084102630615\n","Iteration 585, Training loss = 0.2703101634979248\n","Iteration 586, Training loss = 0.2702483534812927\n","Iteration 587, Training loss = 0.2701881229877472\n","Iteration 588, Training loss = 0.27012673020362854\n","Iteration 589, Training loss = 0.27006691694259644\n","Iteration 590, Training loss = 0.2700059711933136\n","Iteration 591, Training loss = 0.26994654536247253\n","Iteration 592, Training loss = 0.2698860466480255\n","Iteration 593, Training loss = 0.2698270380496979\n","Iteration 594, Training loss = 0.2697669267654419\n","Iteration 595, Training loss = 0.2697083353996277\n","Iteration 596, Training loss = 0.26964861154556274\n","Iteration 597, Training loss = 0.26959043741226196\n","Iteration 598, Training loss = 0.26953113079071045\n","Iteration 599, Training loss = 0.2694733440876007\n","Iteration 600, Training loss = 0.26941442489624023\n","Iteration 601, Training loss = 0.26935702562332153\n","Iteration 602, Training loss = 0.2692984938621521\n","Iteration 603, Training loss = 0.2692415118217468\n","Iteration 604, Training loss = 0.26918336749076843\n","Iteration 605, Training loss = 0.2691267430782318\n","Iteration 606, Training loss = 0.26906901597976685\n","Iteration 607, Training loss = 0.2690127491950989\n","Iteration 608, Training loss = 0.26895537972450256\n","Iteration 609, Training loss = 0.26889950037002563\n","Iteration 610, Training loss = 0.26884251832962036\n","Iteration 611, Training loss = 0.2687870264053345\n","Iteration 612, Training loss = 0.26873040199279785\n","Iteration 613, Training loss = 0.2686752378940582\n","Iteration 614, Training loss = 0.26861900091171265\n","Iteration 615, Training loss = 0.2685641944408417\n","Iteration 616, Training loss = 0.26850831508636475\n","Iteration 617, Training loss = 0.2684538960456848\n","Iteration 618, Training loss = 0.26839834451675415\n","Iteration 619, Training loss = 0.26834428310394287\n","Iteration 620, Training loss = 0.26828908920288086\n","Iteration 621, Training loss = 0.26823535561561584\n","Iteration 622, Training loss = 0.2681805193424225\n","Iteration 623, Training loss = 0.2681271433830261\n","Iteration 624, Training loss = 0.26807263493537903\n","Iteration 625, Training loss = 0.26801958680152893\n","Iteration 626, Training loss = 0.2679654359817505\n","Iteration 627, Training loss = 0.26791277527809143\n","Iteration 628, Training loss = 0.26785892248153687\n","Iteration 629, Training loss = 0.2678065299987793\n","Iteration 630, Training loss = 0.26775306463241577\n","Iteration 631, Training loss = 0.26770099997520447\n","Iteration 632, Training loss = 0.2676478624343872\n","Iteration 633, Training loss = 0.26759615540504456\n","Iteration 634, Training loss = 0.2675432860851288\n","Iteration 635, Training loss = 0.2674919068813324\n","Iteration 636, Training loss = 0.2674393951892853\n","Iteration 637, Training loss = 0.2673882842063904\n","Iteration 638, Training loss = 0.2673361301422119\n","Iteration 639, Training loss = 0.2672853469848633\n","Iteration 640, Training loss = 0.2672334611415863\n","Iteration 641, Training loss = 0.26718300580978394\n","Iteration 642, Training loss = 0.26713141798973083\n","Iteration 643, Training loss = 0.26708126068115234\n","Iteration 644, Training loss = 0.2670300006866455\n","Iteration 645, Training loss = 0.2669801712036133\n","Iteration 646, Training loss = 0.2669292092323303\n","Iteration 647, Training loss = 0.2668796479701996\n","Iteration 648, Training loss = 0.2668290138244629\n","Iteration 649, Training loss = 0.26677972078323364\n","Iteration 650, Training loss = 0.2667293846607208\n","Iteration 651, Training loss = 0.26668038964271545\n","Iteration 652, Training loss = 0.2666303515434265\n","Iteration 653, Training loss = 0.266581654548645\n","Iteration 654, Training loss = 0.26653191447257996\n","Iteration 655, Training loss = 0.26648351550102234\n","Iteration 656, Training loss = 0.2664340138435364\n","Iteration 657, Training loss = 0.26638591289520264\n","Iteration 658, Training loss = 0.26633670926094055\n","Iteration 659, Training loss = 0.2662888765335083\n","Iteration 660, Training loss = 0.2662399709224701\n","Iteration 661, Training loss = 0.26619240641593933\n","Iteration 662, Training loss = 0.2661437690258026\n","Iteration 663, Training loss = 0.2660965025424957\n","Iteration 664, Training loss = 0.2660481333732605\n","Iteration 665, Training loss = 0.2660011053085327\n","Iteration 666, Training loss = 0.26595303416252136\n","Iteration 667, Training loss = 0.26590627431869507\n","Iteration 668, Training loss = 0.2658584713935852\n","Iteration 669, Training loss = 0.2658120095729828\n","Iteration 670, Training loss = 0.265764445066452\n","Iteration 671, Training loss = 0.2657182216644287\n","Iteration 672, Training loss = 0.2656709551811218\n","Iteration 673, Training loss = 0.265625\n","Iteration 674, Training loss = 0.2655779719352722\n","Iteration 675, Training loss = 0.2655322849750519\n","Iteration 676, Training loss = 0.2654855251312256\n","Iteration 677, Training loss = 0.26544007658958435\n","Iteration 678, Training loss = 0.26539355516433716\n","Iteration 679, Training loss = 0.2653483748435974\n","Iteration 680, Training loss = 0.2653021216392517\n","Iteration 681, Training loss = 0.2652571499347687\n","Iteration 682, Training loss = 0.26521116495132446\n","Iteration 683, Training loss = 0.2651664614677429\n","Iteration 684, Training loss = 0.2651207149028778\n","Iteration 685, Training loss = 0.26507624983787537\n","Iteration 686, Training loss = 0.26503077149391174\n","Iteration 687, Training loss = 0.2649865448474884\n","Iteration 688, Training loss = 0.2649412751197815\n","Iteration 689, Training loss = 0.26489731669425964\n","Iteration 690, Training loss = 0.26485228538513184\n","Iteration 691, Training loss = 0.2648085653781891\n","Iteration 692, Training loss = 0.2647637724876404\n","Iteration 693, Training loss = 0.26472029089927673\n","Iteration 694, Training loss = 0.26467570662498474\n","Iteration 695, Training loss = 0.2646324634552002\n","Iteration 696, Training loss = 0.2645881474018097\n","Iteration 697, Training loss = 0.26454511284828186\n","Iteration 698, Training loss = 0.26450103521347046\n","Iteration 699, Training loss = 0.26445823907852173\n","Iteration 700, Training loss = 0.26441437005996704\n","Iteration 701, Training loss = 0.264371782541275\n","Iteration 702, Training loss = 0.26432815194129944\n","Iteration 703, Training loss = 0.2642858028411865\n","Iteration 704, Training loss = 0.26424238085746765\n","Iteration 705, Training loss = 0.26420027017593384\n","Iteration 706, Training loss = 0.2641570568084717\n","Iteration 707, Training loss = 0.2641151547431946\n","Iteration 708, Training loss = 0.2640722095966339\n","Iteration 709, Training loss = 0.2640305161476135\n","Iteration 710, Training loss = 0.2639877498149872\n","Iteration 711, Training loss = 0.2639462649822235\n","Iteration 712, Training loss = 0.26390373706817627\n","Iteration 713, Training loss = 0.2638624608516693\n","Iteration 714, Training loss = 0.2638201415538788\n","Iteration 715, Training loss = 0.2637791037559509\n","Iteration 716, Training loss = 0.2637370228767395\n","Iteration 717, Training loss = 0.26369616389274597\n","Iteration 718, Training loss = 0.26365426182746887\n","Iteration 719, Training loss = 0.26361361145973206\n","Iteration 720, Training loss = 0.26357191801071167\n","Iteration 721, Training loss = 0.26353150606155396\n","Iteration 722, Training loss = 0.2634900212287903\n","Iteration 723, Training loss = 0.2634497582912445\n","Iteration 724, Training loss = 0.26340848207473755\n","Iteration 725, Training loss = 0.26336848735809326\n","Iteration 726, Training loss = 0.263327419757843\n","Iteration 727, Training loss = 0.26328757405281067\n","Iteration 728, Training loss = 0.26324668526649475\n","Iteration 729, Training loss = 0.2632070779800415\n","Iteration 730, Training loss = 0.2631663680076599\n","Iteration 731, Training loss = 0.263126939535141\n","Iteration 732, Training loss = 0.2630864679813385\n","Iteration 733, Training loss = 0.2630472183227539\n","Iteration 734, Training loss = 0.26300695538520813\n","Iteration 735, Training loss = 0.26296791434288025\n","Iteration 736, Training loss = 0.2629278302192688\n","Iteration 737, Training loss = 0.26288893818855286\n","Iteration 738, Training loss = 0.2628490626811981\n","Iteration 739, Training loss = 0.2628103792667389\n","Iteration 740, Training loss = 0.2627706825733185\n","Iteration 741, Training loss = 0.26273220777511597\n","Iteration 742, Training loss = 0.2626926898956299\n","Iteration 743, Training loss = 0.2626543641090393\n","Iteration 744, Training loss = 0.26261505484580994\n","Iteration 745, Training loss = 0.2625769376754761\n","Iteration 746, Training loss = 0.26253777742385864\n","Iteration 747, Training loss = 0.2624998390674591\n","Iteration 748, Training loss = 0.2624609172344208\n","Iteration 749, Training loss = 0.2624231278896332\n","Iteration 750, Training loss = 0.2623843848705292\n","Iteration 751, Training loss = 0.2623467743396759\n","Iteration 752, Training loss = 0.26230818033218384\n","Iteration 753, Training loss = 0.2622707784175873\n","Iteration 754, Training loss = 0.26223236322402954\n","Iteration 755, Training loss = 0.2621951401233673\n","Iteration 756, Training loss = 0.2621569037437439\n","Iteration 757, Training loss = 0.2621198296546936\n","Iteration 758, Training loss = 0.2620817720890045\n","Iteration 759, Training loss = 0.26204487681388855\n","Iteration 760, Training loss = 0.2620070278644562\n","Iteration 761, Training loss = 0.26197031140327454\n","Iteration 762, Training loss = 0.2619325518608093\n","Iteration 763, Training loss = 0.261896014213562\n","Iteration 764, Training loss = 0.2618584930896759\n","Iteration 765, Training loss = 0.2618221342563629\n","Iteration 766, Training loss = 0.26178473234176636\n","Iteration 767, Training loss = 0.2617485225200653\n","Iteration 768, Training loss = 0.26171132922172546\n","Iteration 769, Training loss = 0.26167532801628113\n","Iteration 770, Training loss = 0.26163825392723083\n","Iteration 771, Training loss = 0.26160237193107605\n","Iteration 772, Training loss = 0.26156550645828247\n","Iteration 773, Training loss = 0.261529803276062\n","Iteration 774, Training loss = 0.261493057012558\n","Iteration 775, Training loss = 0.26145756244659424\n","Iteration 776, Training loss = 0.26142096519470215\n","Iteration 777, Training loss = 0.26138558983802795\n","Iteration 778, Training loss = 0.2613492012023926\n","Iteration 779, Training loss = 0.2613139748573303\n","Iteration 780, Training loss = 0.2612777352333069\n","Iteration 781, Training loss = 0.26124265789985657\n","Iteration 782, Training loss = 0.26120659708976746\n","Iteration 783, Training loss = 0.2611716687679291\n","Iteration 784, Training loss = 0.2611357569694519\n","Iteration 785, Training loss = 0.26110100746154785\n","Iteration 786, Training loss = 0.2610652446746826\n","Iteration 787, Training loss = 0.2610306441783905\n","Iteration 788, Training loss = 0.2609950304031372\n","Iteration 789, Training loss = 0.26096057891845703\n","Iteration 790, Training loss = 0.2609251141548157\n","Iteration 791, Training loss = 0.26089081168174744\n","Iteration 792, Training loss = 0.2608555257320404\n","Iteration 793, Training loss = 0.2608213722705841\n","Iteration 794, Training loss = 0.2607862055301666\n","Iteration 795, Training loss = 0.26075220108032227\n","Iteration 796, Training loss = 0.2607172429561615\n","Iteration 797, Training loss = 0.2606833577156067\n","Iteration 798, Training loss = 0.26064854860305786\n","Iteration 799, Training loss = 0.260614812374115\n","Iteration 800, Training loss = 0.2605801224708557\n","Iteration 801, Training loss = 0.2605465352535248\n","Iteration 802, Training loss = 0.26051196455955505\n","Iteration 803, Training loss = 0.26047855615615845\n","Iteration 804, Training loss = 0.26044416427612305\n","Iteration 805, Training loss = 0.260410875082016\n","Iteration 806, Training loss = 0.26037663221359253\n","Iteration 807, Training loss = 0.2603434920310974\n","Iteration 808, Training loss = 0.2603093385696411\n","Iteration 809, Training loss = 0.26027634739875793\n","Iteration 810, Training loss = 0.26024237275123596\n","Iteration 811, Training loss = 0.26020950078964233\n","Iteration 812, Training loss = 0.2601756453514099\n","Iteration 813, Training loss = 0.2601429522037506\n","Iteration 814, Training loss = 0.2601092457771301\n","Iteration 815, Training loss = 0.26007670164108276\n","Iteration 816, Training loss = 0.26004311442375183\n","Iteration 817, Training loss = 0.260010689496994\n","Iteration 818, Training loss = 0.259977251291275\n","Iteration 819, Training loss = 0.25994494557380676\n","Iteration 820, Training loss = 0.2599116265773773\n","Iteration 821, Training loss = 0.259879469871521\n","Iteration 822, Training loss = 0.2598463296890259\n","Iteration 823, Training loss = 0.2598142921924591\n","Iteration 824, Training loss = 0.25978124141693115\n","Iteration 825, Training loss = 0.2597493529319763\n","Iteration 826, Training loss = 0.2597164511680603\n","Iteration 827, Training loss = 0.2596847116947174\n","Iteration 828, Training loss = 0.25965192914009094\n","Iteration 829, Training loss = 0.2596202790737152\n","Iteration 830, Training loss = 0.2595876455307007\n","Iteration 831, Training loss = 0.2595561444759369\n","Iteration 832, Training loss = 0.2595236301422119\n","Iteration 833, Training loss = 0.25949224829673767\n","Iteration 834, Training loss = 0.25945988297462463\n","Iteration 835, Training loss = 0.25942865014076233\n","Iteration 836, Training loss = 0.25939640402793884\n","Iteration 837, Training loss = 0.2593652606010437\n","Iteration 838, Training loss = 0.25933316349983215\n","Iteration 839, Training loss = 0.25930213928222656\n","Iteration 840, Training loss = 0.25927016139030457\n","Iteration 841, Training loss = 0.2592392861843109\n","Iteration 842, Training loss = 0.25920742750167847\n","Iteration 843, Training loss = 0.25917667150497437\n","Iteration 844, Training loss = 0.25914496183395386\n","Iteration 845, Training loss = 0.2591142952442169\n","Iteration 846, Training loss = 0.2590826749801636\n","Iteration 847, Training loss = 0.25905218720436096\n","Iteration 848, Training loss = 0.25902068614959717\n","Iteration 849, Training loss = 0.2589902877807617\n","Iteration 850, Training loss = 0.25895893573760986\n","Iteration 851, Training loss = 0.25892865657806396\n","Iteration 852, Training loss = 0.25889742374420166\n","Iteration 853, Training loss = 0.2588672637939453\n","Iteration 854, Training loss = 0.25883612036705017\n","Iteration 855, Training loss = 0.25880610942840576\n","Iteration 856, Training loss = 0.25877508521080017\n","Iteration 857, Training loss = 0.2587451934814453\n","Iteration 858, Training loss = 0.2587142884731293\n","Iteration 859, Training loss = 0.25868451595306396\n","Iteration 860, Training loss = 0.2586537301540375\n","Iteration 861, Training loss = 0.25862404704093933\n","Iteration 862, Training loss = 0.2585933804512024\n","Iteration 863, Training loss = 0.2585638165473938\n","Iteration 864, Training loss = 0.2585332989692688\n","Iteration 865, Training loss = 0.25850385427474976\n","Iteration 866, Training loss = 0.25847339630126953\n","Iteration 867, Training loss = 0.25844407081604004\n","Iteration 868, Training loss = 0.25841376185417175\n","Iteration 869, Training loss = 0.2583845555782318\n","Iteration 870, Training loss = 0.2583543658256531\n","Iteration 871, Training loss = 0.2583252191543579\n","Iteration 872, Training loss = 0.2582951486110687\n","Iteration 873, Training loss = 0.2582661509513855\n","Iteration 874, Training loss = 0.25823619961738586\n","Iteration 875, Training loss = 0.2582072913646698\n","Iteration 876, Training loss = 0.25817742943763733\n","Iteration 877, Training loss = 0.2581486701965332\n","Iteration 878, Training loss = 0.2581189274787903\n","Iteration 879, Training loss = 0.25809022784233093\n","Iteration 880, Training loss = 0.25806060433387756\n","Iteration 881, Training loss = 0.25803202390670776\n","Iteration 882, Training loss = 0.25800251960754395\n","Iteration 883, Training loss = 0.2579740285873413\n","Iteration 884, Training loss = 0.25794464349746704\n","Iteration 885, Training loss = 0.25791627168655396\n","Iteration 886, Training loss = 0.25788694620132446\n","Iteration 887, Training loss = 0.2578586935997009\n","Iteration 888, Training loss = 0.257829487323761\n","Iteration 889, Training loss = 0.257801353931427\n","Iteration 890, Training loss = 0.2577722668647766\n","Iteration 891, Training loss = 0.2577442228794098\n","Iteration 892, Training loss = 0.25771522521972656\n","Iteration 893, Training loss = 0.2576873004436493\n","Iteration 894, Training loss = 0.2576584219932556\n","Iteration 895, Training loss = 0.2576305866241455\n","Iteration 896, Training loss = 0.257601797580719\n","Iteration 897, Training loss = 0.25757408142089844\n","Iteration 898, Training loss = 0.2575453817844391\n","Iteration 899, Training loss = 0.2575177848339081\n","Iteration 900, Training loss = 0.2574891746044159\n","Iteration 901, Training loss = 0.25746166706085205\n","Iteration 902, Training loss = 0.2574331760406494\n","Iteration 903, Training loss = 0.25740575790405273\n","Iteration 904, Training loss = 0.25737738609313965\n","Iteration 905, Training loss = 0.25735002756118774\n","Iteration 906, Training loss = 0.2573218047618866\n","Iteration 907, Training loss = 0.25729453563690186\n","Iteration 908, Training loss = 0.2572663724422455\n","Iteration 909, Training loss = 0.2572392523288727\n","Iteration 910, Training loss = 0.25721117854118347\n","Iteration 911, Training loss = 0.25718414783477783\n","Iteration 912, Training loss = 0.2571561634540558\n","Iteration 913, Training loss = 0.2571292221546173\n","Iteration 914, Training loss = 0.2571013569831848\n","Iteration 915, Training loss = 0.2570745050907135\n","Iteration 916, Training loss = 0.25704675912857056\n","Iteration 917, Training loss = 0.2570199966430664\n","Iteration 918, Training loss = 0.25699231028556824\n","Iteration 919, Training loss = 0.25696566700935364\n","Iteration 920, Training loss = 0.256938099861145\n","Iteration 921, Training loss = 0.2569115161895752\n","Iteration 922, Training loss = 0.25688403844833374\n","Iteration 923, Training loss = 0.25685757398605347\n","Iteration 924, Training loss = 0.2568301856517792\n","Iteration 925, Training loss = 0.25680381059646606\n","Iteration 926, Training loss = 0.25677651166915894\n","Iteration 927, Training loss = 0.2567502558231354\n","Iteration 928, Training loss = 0.256723016500473\n","Iteration 929, Training loss = 0.2566968500614166\n","Iteration 930, Training loss = 0.2566697299480438\n","Iteration 931, Training loss = 0.2566436529159546\n","Iteration 932, Training loss = 0.25661665201187134\n","Iteration 933, Training loss = 0.2565906345844269\n","Iteration 934, Training loss = 0.2565636932849884\n","Iteration 935, Training loss = 0.2565377950668335\n","Iteration 936, Training loss = 0.2565109431743622\n","Iteration 937, Training loss = 0.25648513436317444\n","Iteration 938, Training loss = 0.2564583718776703\n","Iteration 939, Training loss = 0.2564326524734497\n","Iteration 940, Training loss = 0.2564060091972351\n","Iteration 941, Training loss = 0.2563803791999817\n","Iteration 942, Training loss = 0.25635379552841187\n","Iteration 943, Training loss = 0.2563282549381256\n","Iteration 944, Training loss = 0.25630176067352295\n","Iteration 945, Training loss = 0.25627630949020386\n","Iteration 946, Training loss = 0.25624993443489075\n","Iteration 947, Training loss = 0.25622454285621643\n","Iteration 948, Training loss = 0.2561982274055481\n","Iteration 949, Training loss = 0.25617295503616333\n","Iteration 950, Training loss = 0.25614675879478455\n","Iteration 951, Training loss = 0.25612154603004456\n","Iteration 952, Training loss = 0.25609543919563293\n","Iteration 953, Training loss = 0.2560703158378601\n","Iteration 954, Training loss = 0.25604426860809326\n","Iteration 955, Training loss = 0.2560192346572876\n","Iteration 956, Training loss = 0.2559932768344879\n","Iteration 957, Training loss = 0.2559683322906494\n","Iteration 958, Training loss = 0.2559424638748169\n","Iteration 959, Training loss = 0.25591760873794556\n","Iteration 960, Training loss = 0.2558917999267578\n","Iteration 961, Training loss = 0.25586703419685364\n","Iteration 962, Training loss = 0.25584134459495544\n","Iteration 963, Training loss = 0.25581663846969604\n","Iteration 964, Training loss = 0.2557910084724426\n","Iteration 965, Training loss = 0.2557663917541504\n","Iteration 966, Training loss = 0.2557408809661865\n","Iteration 967, Training loss = 0.25571635365486145\n","Iteration 968, Training loss = 0.25569090247154236\n","Iteration 969, Training loss = 0.25566646456718445\n","Iteration 970, Training loss = 0.25564107298851013\n","Iteration 971, Training loss = 0.2556167244911194\n","Iteration 972, Training loss = 0.2555914521217346\n","Iteration 973, Training loss = 0.25556713342666626\n","Iteration 974, Training loss = 0.25554195046424866\n","Iteration 975, Training loss = 0.25551775097846985\n","Iteration 976, Training loss = 0.255492627620697\n","Iteration 977, Training loss = 0.255468487739563\n","Iteration 978, Training loss = 0.2554434537887573\n","Iteration 979, Training loss = 0.25541937351226807\n","Iteration 980, Training loss = 0.25539442896842957\n","Iteration 981, Training loss = 0.25537046790122986\n","Iteration 982, Training loss = 0.25534558296203613\n","Iteration 983, Training loss = 0.2553216814994812\n","Iteration 984, Training loss = 0.25529688596725464\n","Iteration 985, Training loss = 0.25527307391166687\n","Iteration 986, Training loss = 0.25524836778640747\n","Iteration 987, Training loss = 0.2552246153354645\n","Iteration 988, Training loss = 0.25519996881484985\n","Iteration 989, Training loss = 0.255176305770874\n","Iteration 990, Training loss = 0.25515174865722656\n","Iteration 991, Training loss = 0.2551281452178955\n","Iteration 992, Training loss = 0.2551036775112152\n","Iteration 993, Training loss = 0.25508013367652893\n","Iteration 994, Training loss = 0.2550557553768158\n","Iteration 995, Training loss = 0.2550323009490967\n","Iteration 996, Training loss = 0.2550079822540283\n","Iteration 997, Training loss = 0.25498461723327637\n","Iteration 998, Training loss = 0.2549603581428528\n","Iteration 999, Training loss = 0.254937082529068\n","Iteration 1000, Training loss = 0.2549128830432892\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Plot training loss\n","\n","---"],"metadata":{"id":"kjIbnCSR57KY"},"id":"kjIbnCSR57KY"},{"cell_type":"code","source":["## Plot the training loss\n","fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n","ax.plot(loss_train, 'b', label = 'Train')\n","ax.set_xlabel('Iteration')\n","ax.set_ylabel('Loss')\n","ax.legend()"],"metadata":{"id":"bg-a6lmZ5-G0","colab":{"base_uri":"https://localhost:8080/","height":405},"executionInfo":{"status":"ok","timestamp":1729229156232,"user_tz":-330,"elapsed":652,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"d938212a-f74d-408c-a725-d500de307938"},"id":"bg-a6lmZ5-G0","execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7914b7ad98a0>"]},"metadata":{},"execution_count":96},{"output_type":"display_data","data":{"text/plain":["<Figure size 400x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["## Assess model performance on test data\n","Yhat = model.predict(X_test_reshaped_scaled)\n","\n","ypred = np.argmax(Yhat, axis = 1) #predicted labels\n","ytrue = np.asarray(np.argmax(Y_test, axis=1)).flatten() #true labels\n","print('Accuracy on test data = %3.2f'%(np.mean(ytrue == ypred)*100))\n","# Print confusion matrix\n","print(confusion_matrix(ytrue, ypred))"],"metadata":{"id":"Du2AnDcb1opu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729229160591,"user_tz":-330,"elapsed":1117,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"a1bcc423-d9b2-44f4-9e08-96701d1dffcc"},"id":"Du2AnDcb1opu","execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","Accuracy on test data = 92.67\n","[[ 962    0    1    2    0    5    6    3    1    0]\n"," [   0 1111    3    2    0    1    4    2   12    0]\n"," [   6    9  924   16   10    3   13    9   37    5]\n"," [   3    0   19  923    0   22    2   11   23    7]\n"," [   1    2    5    2  918    0   10    3    9   32]\n"," [   9    2    3   35   10  771   15    9   32    6]\n"," [   9    3    7    1    9   13  912    2    2    0]\n"," [   1    6   23    5    7    1    0  953    3   29]\n"," [   6    9    5   20    9   22    9   12  877    5]\n"," [  11    8    1    9   26    7    0   25    6  916]]\n"]}]},{"cell_type":"code","source":["## Plot a random test sample with its predicted label printed above the plot\n","test_index = np.random.choice(X_test.shape[0])\n","fig, ax = plt.subplots(1, 1, figsize = (2, 2))\n","print(f'Image classified as {ypred[test_index]}')\n","ax.imshow(tf.reshape(X_test_reshaped_scaled[test_index], [28, 28]).numpy(), cmap = 'gray');"],"metadata":{"id":"9MXEKezr3i4l","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"ok","timestamp":1729229175731,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sathvik Nayak","userId":"11086093538711749215"}},"outputId":"8273d99f-444c-4e10-ddaf-c6bf6c86d64d"},"id":"9MXEKezr3i4l","execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["Image classified as 2\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 200x200 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}